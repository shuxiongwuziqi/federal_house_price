{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data from CSV, perform one-hot encoding and normalization processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data():\n",
    "    num_cols = [\n",
    "        \"number of rooms\",\n",
    "        \"security level of the community\",\n",
    "        \"residence space\",\n",
    "        \"noise level\",\n",
    "        \"waterfront\",\n",
    "        \"view\",\n",
    "        \"air quality level\",\n",
    "        \"aboveground space\",\n",
    "        \"building year\",\n",
    "        \"decoration year\",\n",
    "        \"lat\",\n",
    "        \"lng\",\n",
    "    ]\n",
    "    cat_cols = [\n",
    "        \"city\",\n",
    "        \"zip code\",\n",
    "    ]\n",
    "\n",
    "    feature_names = cat_cols + num_cols\n",
    "    df = pd.read_csv(\"./Train_Data.csv\")\n",
    "    train_length = len(df)\n",
    "    df_test = pd.read_csv(\"./Test_Data.csv\")\n",
    "    df_all = pd.concat([df, df_test], axis=0)\n",
    "    \n",
    "    features = df_all[feature_names]\n",
    "    features = pd.get_dummies(features, columns=cat_cols)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features[num_cols] = scaler.fit_transform(features[num_cols])\n",
    "\n",
    "    features = features.to_numpy()\n",
    "    [features, test_features] = [features[:train_length], features[train_length:]]\n",
    "    labels = df[\"label\"].to_numpy()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.1, random_state=42)\n",
    "    return (x_train, y_train), (x_test, y_test), test_features\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), test_features = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7845662799860171\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=260, max_features = 0.3)\n",
    "model.fit(x_train, y_train)\n",
    "accuracy = model.score(x_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define MyModel to meet the training of feature to label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (fc1): Linear(in_features=133, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义模型类\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(133, 64)\n",
    "        self.fc2 = nn.Linear(64, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel()\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define MyDataset to meet local learning requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.x[index]).to(torch.float32), torch.tensor(self.y[index])\n",
    "\n",
    "batchsize = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset = MyDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The accuracy of traditional training methods is about 76%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, testloader):\n",
    "    \"\"\" Returns the inference accuracy and loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "    for batch_idx, (features, labels) in enumerate(testloader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Inference\n",
    "        outputs = model(features)\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Prediction\n",
    "        _, pred_labels = torch.max(outputs, 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "    loss /= batch_idx\n",
    "    accuracy = correct/total\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 | [0/3600 (0%)]\tLoss: 1.390053\n",
      "Epoch : 0 | [1280/3600 (34%)]\tLoss: 1.377730\n",
      "Epoch : 0 | [2560/3600 (69%)]\tLoss: 1.364466\n",
      "Epoch : 1 | [0/3600 (0%)]\tLoss: 1.352945\n",
      "Epoch : 1 | [1280/3600 (34%)]\tLoss: 1.322260\n",
      "Epoch : 1 | [2560/3600 (69%)]\tLoss: 1.308271\n",
      "Epoch : 2 | [0/3600 (0%)]\tLoss: 1.260534\n",
      "Epoch : 2 | [1280/3600 (34%)]\tLoss: 1.229498\n",
      "Epoch : 2 | [2560/3600 (69%)]\tLoss: 1.219065\n",
      "Epoch : 3 | [0/3600 (0%)]\tLoss: 1.169868\n",
      "Epoch : 3 | [1280/3600 (34%)]\tLoss: 1.167930\n",
      "Epoch : 3 | [2560/3600 (69%)]\tLoss: 1.179945\n",
      "Epoch : 4 | [0/3600 (0%)]\tLoss: 1.153902\n",
      "Epoch : 4 | [1280/3600 (34%)]\tLoss: 1.112089\n",
      "Epoch : 4 | [2560/3600 (69%)]\tLoss: 1.133177\n",
      "Epoch : 5 | [0/3600 (0%)]\tLoss: 1.162403\n",
      "Epoch : 5 | [1280/3600 (34%)]\tLoss: 1.124571\n",
      "Epoch : 5 | [2560/3600 (69%)]\tLoss: 1.110529\n",
      "Epoch : 6 | [0/3600 (0%)]\tLoss: 1.071399\n",
      "Epoch : 6 | [1280/3600 (34%)]\tLoss: 1.054229\n",
      "Epoch : 6 | [2560/3600 (69%)]\tLoss: 1.076235\n",
      "Epoch : 7 | [0/3600 (0%)]\tLoss: 1.075622\n",
      "Epoch : 7 | [1280/3600 (34%)]\tLoss: 1.036023\n",
      "Epoch : 7 | [2560/3600 (69%)]\tLoss: 1.104293\n",
      "Epoch : 8 | [0/3600 (0%)]\tLoss: 1.060278\n",
      "Epoch : 8 | [1280/3600 (34%)]\tLoss: 1.027351\n",
      "Epoch : 8 | [2560/3600 (69%)]\tLoss: 1.053541\n",
      "Epoch : 9 | [0/3600 (0%)]\tLoss: 1.068210\n",
      "Epoch : 9 | [1280/3600 (34%)]\tLoss: 1.048904\n",
      "Epoch : 9 | [2560/3600 (69%)]\tLoss: 1.029915\n",
      "Epoch : 10 | [0/3600 (0%)]\tLoss: 1.020948\n",
      "Epoch : 10 | [1280/3600 (34%)]\tLoss: 1.044977\n",
      "Epoch : 10 | [2560/3600 (69%)]\tLoss: 1.003164\n",
      "Epoch : 11 | [0/3600 (0%)]\tLoss: 1.027852\n",
      "Epoch : 11 | [1280/3600 (34%)]\tLoss: 1.015479\n",
      "Epoch : 11 | [2560/3600 (69%)]\tLoss: 1.017382\n",
      "Epoch : 12 | [0/3600 (0%)]\tLoss: 1.038478\n",
      "Epoch : 12 | [1280/3600 (34%)]\tLoss: 1.018933\n",
      "Epoch : 12 | [2560/3600 (69%)]\tLoss: 1.017015\n",
      "Epoch : 13 | [0/3600 (0%)]\tLoss: 1.046306\n",
      "Epoch : 13 | [1280/3600 (34%)]\tLoss: 0.973436\n",
      "Epoch : 13 | [2560/3600 (69%)]\tLoss: 0.989803\n",
      "Epoch : 14 | [0/3600 (0%)]\tLoss: 1.005218\n",
      "Epoch : 14 | [1280/3600 (34%)]\tLoss: 0.991862\n",
      "Epoch : 14 | [2560/3600 (69%)]\tLoss: 1.035623\n",
      "Epoch : 15 | [0/3600 (0%)]\tLoss: 1.026621\n",
      "Epoch : 15 | [1280/3600 (34%)]\tLoss: 1.036064\n",
      "Epoch : 15 | [2560/3600 (69%)]\tLoss: 0.967804\n",
      "Epoch : 16 | [0/3600 (0%)]\tLoss: 1.001821\n",
      "Epoch : 16 | [1280/3600 (34%)]\tLoss: 1.000222\n",
      "Epoch : 16 | [2560/3600 (69%)]\tLoss: 1.057758\n",
      "Epoch : 17 | [0/3600 (0%)]\tLoss: 0.991661\n",
      "Epoch : 17 | [1280/3600 (34%)]\tLoss: 1.024639\n",
      "Epoch : 17 | [2560/3600 (69%)]\tLoss: 1.032881\n",
      "Epoch : 18 | [0/3600 (0%)]\tLoss: 1.013904\n",
      "Epoch : 18 | [1280/3600 (34%)]\tLoss: 0.984502\n",
      "Epoch : 18 | [2560/3600 (69%)]\tLoss: 0.982781\n",
      "Epoch : 19 | [0/3600 (0%)]\tLoss: 1.001047\n",
      "Epoch : 19 | [1280/3600 (34%)]\tLoss: 0.957570\n",
      "Epoch : 19 | [2560/3600 (69%)]\tLoss: 1.011629\n",
      "Epoch : 20 | [0/3600 (0%)]\tLoss: 1.069823\n",
      "Epoch : 20 | [1280/3600 (34%)]\tLoss: 1.024519\n",
      "Epoch : 20 | [2560/3600 (69%)]\tLoss: 1.013535\n",
      "Epoch : 21 | [0/3600 (0%)]\tLoss: 0.998396\n",
      "Epoch : 21 | [1280/3600 (34%)]\tLoss: 0.965876\n",
      "Epoch : 21 | [2560/3600 (69%)]\tLoss: 0.983732\n",
      "Epoch : 22 | [0/3600 (0%)]\tLoss: 1.011089\n",
      "Epoch : 22 | [1280/3600 (34%)]\tLoss: 0.984587\n",
      "Epoch : 22 | [2560/3600 (69%)]\tLoss: 1.041983\n",
      "Epoch : 23 | [0/3600 (0%)]\tLoss: 1.011338\n",
      "Epoch : 23 | [1280/3600 (34%)]\tLoss: 1.030244\n",
      "Epoch : 23 | [2560/3600 (69%)]\tLoss: 1.005403\n",
      "Epoch : 24 | [0/3600 (0%)]\tLoss: 1.019722\n",
      "Epoch : 24 | [1280/3600 (34%)]\tLoss: 0.985629\n",
      "Epoch : 24 | [2560/3600 (69%)]\tLoss: 0.965681\n",
      "Epoch : 25 | [0/3600 (0%)]\tLoss: 0.967328\n",
      "Epoch : 25 | [1280/3600 (34%)]\tLoss: 0.981472\n",
      "Epoch : 25 | [2560/3600 (69%)]\tLoss: 1.010564\n",
      "Epoch : 26 | [0/3600 (0%)]\tLoss: 0.955516\n",
      "Epoch : 26 | [1280/3600 (34%)]\tLoss: 0.987069\n",
      "Epoch : 26 | [2560/3600 (69%)]\tLoss: 1.022675\n",
      "Epoch : 27 | [0/3600 (0%)]\tLoss: 1.003371\n",
      "Epoch : 27 | [1280/3600 (34%)]\tLoss: 0.974765\n",
      "Epoch : 27 | [2560/3600 (69%)]\tLoss: 0.997753\n",
      "Epoch : 28 | [0/3600 (0%)]\tLoss: 0.927192\n",
      "Epoch : 28 | [1280/3600 (34%)]\tLoss: 0.994104\n",
      "Epoch : 28 | [2560/3600 (69%)]\tLoss: 0.944633\n",
      "Epoch : 29 | [0/3600 (0%)]\tLoss: 0.938282\n",
      "Epoch : 29 | [1280/3600 (34%)]\tLoss: 0.982206\n",
      "Epoch : 29 | [2560/3600 (69%)]\tLoss: 0.995874\n",
      "Epoch : 30 | [0/3600 (0%)]\tLoss: 0.974750\n",
      "Epoch : 30 | [1280/3600 (34%)]\tLoss: 0.991723\n",
      "Epoch : 30 | [2560/3600 (69%)]\tLoss: 1.054852\n",
      "Epoch : 31 | [0/3600 (0%)]\tLoss: 0.906083\n",
      "Epoch : 31 | [1280/3600 (34%)]\tLoss: 1.033466\n",
      "Epoch : 31 | [2560/3600 (69%)]\tLoss: 0.976044\n",
      "Epoch : 32 | [0/3600 (0%)]\tLoss: 0.886776\n",
      "Epoch : 32 | [1280/3600 (34%)]\tLoss: 0.995856\n",
      "Epoch : 32 | [2560/3600 (69%)]\tLoss: 0.979853\n",
      "Epoch : 33 | [0/3600 (0%)]\tLoss: 0.937917\n",
      "Epoch : 33 | [1280/3600 (34%)]\tLoss: 1.018708\n",
      "Epoch : 33 | [2560/3600 (69%)]\tLoss: 0.985923\n",
      "Epoch : 34 | [0/3600 (0%)]\tLoss: 1.015903\n",
      "Epoch : 34 | [1280/3600 (34%)]\tLoss: 1.019618\n",
      "Epoch : 34 | [2560/3600 (69%)]\tLoss: 0.895341\n",
      "Epoch : 35 | [0/3600 (0%)]\tLoss: 0.969569\n",
      "Epoch : 35 | [1280/3600 (34%)]\tLoss: 0.939315\n",
      "Epoch : 35 | [2560/3600 (69%)]\tLoss: 0.974815\n",
      "Epoch : 36 | [0/3600 (0%)]\tLoss: 0.930720\n",
      "Epoch : 36 | [1280/3600 (34%)]\tLoss: 0.942062\n",
      "Epoch : 36 | [2560/3600 (69%)]\tLoss: 0.974112\n",
      "Epoch : 37 | [0/3600 (0%)]\tLoss: 0.934370\n",
      "Epoch : 37 | [1280/3600 (34%)]\tLoss: 0.975173\n",
      "Epoch : 37 | [2560/3600 (69%)]\tLoss: 0.960554\n",
      "Epoch : 38 | [0/3600 (0%)]\tLoss: 0.947320\n",
      "Epoch : 38 | [1280/3600 (34%)]\tLoss: 0.964853\n",
      "Epoch : 38 | [2560/3600 (69%)]\tLoss: 0.990789\n",
      "Epoch : 39 | [0/3600 (0%)]\tLoss: 0.977075\n",
      "Epoch : 39 | [1280/3600 (34%)]\tLoss: 0.985005\n",
      "Epoch : 39 | [2560/3600 (69%)]\tLoss: 0.995486\n",
      "Epoch : 40 | [0/3600 (0%)]\tLoss: 0.973024\n",
      "Epoch : 40 | [1280/3600 (34%)]\tLoss: 0.952110\n",
      "Epoch : 40 | [2560/3600 (69%)]\tLoss: 0.970439\n",
      "Epoch : 41 | [0/3600 (0%)]\tLoss: 0.932670\n",
      "Epoch : 41 | [1280/3600 (34%)]\tLoss: 0.946536\n",
      "Epoch : 41 | [2560/3600 (69%)]\tLoss: 1.008316\n",
      "Epoch : 42 | [0/3600 (0%)]\tLoss: 0.958747\n",
      "Epoch : 42 | [1280/3600 (34%)]\tLoss: 0.988225\n",
      "Epoch : 42 | [2560/3600 (69%)]\tLoss: 1.009765\n",
      "Epoch : 43 | [0/3600 (0%)]\tLoss: 0.952540\n",
      "Epoch : 43 | [1280/3600 (34%)]\tLoss: 0.913877\n",
      "Epoch : 43 | [2560/3600 (69%)]\tLoss: 0.934217\n",
      "Epoch : 44 | [0/3600 (0%)]\tLoss: 1.007171\n",
      "Epoch : 44 | [1280/3600 (34%)]\tLoss: 0.945745\n",
      "Epoch : 44 | [2560/3600 (69%)]\tLoss: 0.967615\n",
      "Epoch : 45 | [0/3600 (0%)]\tLoss: 0.930464\n",
      "Epoch : 45 | [1280/3600 (34%)]\tLoss: 0.977663\n",
      "Epoch : 45 | [2560/3600 (69%)]\tLoss: 0.989909\n",
      "Epoch : 46 | [0/3600 (0%)]\tLoss: 0.978036\n",
      "Epoch : 46 | [1280/3600 (34%)]\tLoss: 0.902408\n",
      "Epoch : 46 | [2560/3600 (69%)]\tLoss: 0.953505\n",
      "Epoch : 47 | [0/3600 (0%)]\tLoss: 0.920436\n",
      "Epoch : 47 | [1280/3600 (34%)]\tLoss: 0.993919\n",
      "Epoch : 47 | [2560/3600 (69%)]\tLoss: 0.952906\n",
      "Epoch : 48 | [0/3600 (0%)]\tLoss: 0.908688\n",
      "Epoch : 48 | [1280/3600 (34%)]\tLoss: 0.952943\n",
      "Epoch : 48 | [2560/3600 (69%)]\tLoss: 0.939758\n",
      "Epoch : 49 | [0/3600 (0%)]\tLoss: 0.968251\n",
      "Epoch : 49 | [1280/3600 (34%)]\tLoss: 0.912878\n",
      "Epoch : 49 | [2560/3600 (69%)]\tLoss: 0.935653\n",
      "Epoch : 50 | [0/3600 (0%)]\tLoss: 0.979898\n",
      "Epoch : 50 | [1280/3600 (34%)]\tLoss: 0.946781\n",
      "Epoch : 50 | [2560/3600 (69%)]\tLoss: 0.953640\n",
      "Epoch : 51 | [0/3600 (0%)]\tLoss: 0.977718\n",
      "Epoch : 51 | [1280/3600 (34%)]\tLoss: 0.948605\n",
      "Epoch : 51 | [2560/3600 (69%)]\tLoss: 0.997909\n",
      "Epoch : 52 | [0/3600 (0%)]\tLoss: 0.962480\n",
      "Epoch : 52 | [1280/3600 (34%)]\tLoss: 0.955635\n",
      "Epoch : 52 | [2560/3600 (69%)]\tLoss: 0.975196\n",
      "Epoch : 53 | [0/3600 (0%)]\tLoss: 0.896073\n",
      "Epoch : 53 | [1280/3600 (34%)]\tLoss: 0.962482\n",
      "Epoch : 53 | [2560/3600 (69%)]\tLoss: 0.986914\n",
      "Epoch : 54 | [0/3600 (0%)]\tLoss: 0.955390\n",
      "Epoch : 54 | [1280/3600 (34%)]\tLoss: 0.936061\n",
      "Epoch : 54 | [2560/3600 (69%)]\tLoss: 0.926069\n",
      "Epoch : 55 | [0/3600 (0%)]\tLoss: 0.978275\n",
      "Epoch : 55 | [1280/3600 (34%)]\tLoss: 0.989731\n",
      "Epoch : 55 | [2560/3600 (69%)]\tLoss: 0.932223\n",
      "Epoch : 56 | [0/3600 (0%)]\tLoss: 0.927920\n",
      "Epoch : 56 | [1280/3600 (34%)]\tLoss: 0.917023\n",
      "Epoch : 56 | [2560/3600 (69%)]\tLoss: 0.880992\n",
      "Epoch : 57 | [0/3600 (0%)]\tLoss: 0.892531\n",
      "Epoch : 57 | [1280/3600 (34%)]\tLoss: 0.977106\n",
      "Epoch : 57 | [2560/3600 (69%)]\tLoss: 0.915610\n",
      "Epoch : 58 | [0/3600 (0%)]\tLoss: 0.973515\n",
      "Epoch : 58 | [1280/3600 (34%)]\tLoss: 0.961095\n",
      "Epoch : 58 | [2560/3600 (69%)]\tLoss: 0.950402\n",
      "Epoch : 59 | [0/3600 (0%)]\tLoss: 0.977121\n",
      "Epoch : 59 | [1280/3600 (34%)]\tLoss: 0.901759\n",
      "Epoch : 59 | [2560/3600 (69%)]\tLoss: 0.967040\n",
      "Epoch : 60 | [0/3600 (0%)]\tLoss: 0.976895\n",
      "Epoch : 60 | [1280/3600 (34%)]\tLoss: 0.948468\n",
      "Epoch : 60 | [2560/3600 (69%)]\tLoss: 0.931770\n",
      "Epoch : 61 | [0/3600 (0%)]\tLoss: 0.953065\n",
      "Epoch : 61 | [1280/3600 (34%)]\tLoss: 0.945920\n",
      "Epoch : 61 | [2560/3600 (69%)]\tLoss: 0.930681\n",
      "Epoch : 62 | [0/3600 (0%)]\tLoss: 0.968446\n",
      "Epoch : 62 | [1280/3600 (34%)]\tLoss: 0.952373\n",
      "Epoch : 62 | [2560/3600 (69%)]\tLoss: 0.893003\n",
      "Epoch : 63 | [0/3600 (0%)]\tLoss: 0.924690\n",
      "Epoch : 63 | [1280/3600 (34%)]\tLoss: 0.917792\n",
      "Epoch : 63 | [2560/3600 (69%)]\tLoss: 0.928358\n",
      "Epoch : 64 | [0/3600 (0%)]\tLoss: 0.972654\n",
      "Epoch : 64 | [1280/3600 (34%)]\tLoss: 0.985619\n",
      "Epoch : 64 | [2560/3600 (69%)]\tLoss: 0.901241\n",
      "Epoch : 65 | [0/3600 (0%)]\tLoss: 0.929497\n",
      "Epoch : 65 | [1280/3600 (34%)]\tLoss: 0.964072\n",
      "Epoch : 65 | [2560/3600 (69%)]\tLoss: 0.906800\n",
      "Epoch : 66 | [0/3600 (0%)]\tLoss: 0.956715\n",
      "Epoch : 66 | [1280/3600 (34%)]\tLoss: 0.954598\n",
      "Epoch : 66 | [2560/3600 (69%)]\tLoss: 0.919669\n",
      "Epoch : 67 | [0/3600 (0%)]\tLoss: 0.955222\n",
      "Epoch : 67 | [1280/3600 (34%)]\tLoss: 0.915530\n",
      "Epoch : 67 | [2560/3600 (69%)]\tLoss: 0.955017\n",
      "Epoch : 68 | [0/3600 (0%)]\tLoss: 0.927886\n",
      "Epoch : 68 | [1280/3600 (34%)]\tLoss: 0.972249\n",
      "Epoch : 68 | [2560/3600 (69%)]\tLoss: 0.950831\n",
      "Epoch : 69 | [0/3600 (0%)]\tLoss: 0.952466\n",
      "Epoch : 69 | [1280/3600 (34%)]\tLoss: 0.910188\n",
      "Epoch : 69 | [2560/3600 (69%)]\tLoss: 0.919609\n",
      "Epoch : 70 | [0/3600 (0%)]\tLoss: 0.936347\n",
      "Epoch : 70 | [1280/3600 (34%)]\tLoss: 0.927184\n",
      "Epoch : 70 | [2560/3600 (69%)]\tLoss: 0.947527\n",
      "Epoch : 71 | [0/3600 (0%)]\tLoss: 0.939191\n",
      "Epoch : 71 | [1280/3600 (34%)]\tLoss: 0.892506\n",
      "Epoch : 71 | [2560/3600 (69%)]\tLoss: 0.948899\n",
      "Epoch : 72 | [0/3600 (0%)]\tLoss: 0.959525\n",
      "Epoch : 72 | [1280/3600 (34%)]\tLoss: 0.932731\n",
      "Epoch : 72 | [2560/3600 (69%)]\tLoss: 0.928017\n",
      "Epoch : 73 | [0/3600 (0%)]\tLoss: 0.935371\n",
      "Epoch : 73 | [1280/3600 (34%)]\tLoss: 0.944572\n",
      "Epoch : 73 | [2560/3600 (69%)]\tLoss: 0.941344\n",
      "Epoch : 74 | [0/3600 (0%)]\tLoss: 0.888996\n",
      "Epoch : 74 | [1280/3600 (34%)]\tLoss: 0.915121\n",
      "Epoch : 74 | [2560/3600 (69%)]\tLoss: 0.949407\n",
      "Epoch : 75 | [0/3600 (0%)]\tLoss: 0.916666\n",
      "Epoch : 75 | [1280/3600 (34%)]\tLoss: 0.983133\n",
      "Epoch : 75 | [2560/3600 (69%)]\tLoss: 0.910496\n",
      "Epoch : 76 | [0/3600 (0%)]\tLoss: 0.940883\n",
      "Epoch : 76 | [1280/3600 (34%)]\tLoss: 0.922858\n",
      "Epoch : 76 | [2560/3600 (69%)]\tLoss: 0.910280\n",
      "Epoch : 77 | [0/3600 (0%)]\tLoss: 0.942670\n",
      "Epoch : 77 | [1280/3600 (34%)]\tLoss: 0.951628\n",
      "Epoch : 77 | [2560/3600 (69%)]\tLoss: 0.919030\n",
      "Epoch : 78 | [0/3600 (0%)]\tLoss: 0.978211\n",
      "Epoch : 78 | [1280/3600 (34%)]\tLoss: 0.935030\n",
      "Epoch : 78 | [2560/3600 (69%)]\tLoss: 0.920798\n",
      "Epoch : 79 | [0/3600 (0%)]\tLoss: 0.932936\n",
      "Epoch : 79 | [1280/3600 (34%)]\tLoss: 0.893710\n",
      "Epoch : 79 | [2560/3600 (69%)]\tLoss: 0.919943\n",
      "Epoch : 80 | [0/3600 (0%)]\tLoss: 0.926768\n",
      "Epoch : 80 | [1280/3600 (34%)]\tLoss: 0.896959\n",
      "Epoch : 80 | [2560/3600 (69%)]\tLoss: 0.965716\n",
      "Epoch : 81 | [0/3600 (0%)]\tLoss: 0.896712\n",
      "Epoch : 81 | [1280/3600 (34%)]\tLoss: 0.931951\n",
      "Epoch : 81 | [2560/3600 (69%)]\tLoss: 0.964351\n",
      "Epoch : 82 | [0/3600 (0%)]\tLoss: 0.918278\n",
      "Epoch : 82 | [1280/3600 (34%)]\tLoss: 0.908520\n",
      "Epoch : 82 | [2560/3600 (69%)]\tLoss: 0.940640\n",
      "Epoch : 83 | [0/3600 (0%)]\tLoss: 0.905557\n",
      "Epoch : 83 | [1280/3600 (34%)]\tLoss: 0.958939\n",
      "Epoch : 83 | [2560/3600 (69%)]\tLoss: 0.912263\n",
      "Epoch : 84 | [0/3600 (0%)]\tLoss: 0.933373\n",
      "Epoch : 84 | [1280/3600 (34%)]\tLoss: 0.979647\n",
      "Epoch : 84 | [2560/3600 (69%)]\tLoss: 0.946384\n",
      "Epoch : 85 | [0/3600 (0%)]\tLoss: 0.946128\n",
      "Epoch : 85 | [1280/3600 (34%)]\tLoss: 0.933092\n",
      "Epoch : 85 | [2560/3600 (69%)]\tLoss: 0.894099\n",
      "Epoch : 86 | [0/3600 (0%)]\tLoss: 0.882952\n",
      "Epoch : 86 | [1280/3600 (34%)]\tLoss: 0.922597\n",
      "Epoch : 86 | [2560/3600 (69%)]\tLoss: 0.927760\n",
      "Epoch : 87 | [0/3600 (0%)]\tLoss: 0.936340\n",
      "Epoch : 87 | [1280/3600 (34%)]\tLoss: 0.984829\n",
      "Epoch : 87 | [2560/3600 (69%)]\tLoss: 0.965461\n",
      "Epoch : 88 | [0/3600 (0%)]\tLoss: 0.920088\n",
      "Epoch : 88 | [1280/3600 (34%)]\tLoss: 0.956340\n",
      "Epoch : 88 | [2560/3600 (69%)]\tLoss: 0.920602\n",
      "Epoch : 89 | [0/3600 (0%)]\tLoss: 0.949272\n",
      "Epoch : 89 | [1280/3600 (34%)]\tLoss: 0.922151\n",
      "Epoch : 89 | [2560/3600 (69%)]\tLoss: 0.926730\n",
      "Epoch : 90 | [0/3600 (0%)]\tLoss: 0.969425\n",
      "Epoch : 90 | [1280/3600 (34%)]\tLoss: 0.893521\n",
      "Epoch : 90 | [2560/3600 (69%)]\tLoss: 0.904182\n",
      "Epoch : 91 | [0/3600 (0%)]\tLoss: 0.943640\n",
      "Epoch : 91 | [1280/3600 (34%)]\tLoss: 0.910553\n",
      "Epoch : 91 | [2560/3600 (69%)]\tLoss: 0.934310\n",
      "Epoch : 92 | [0/3600 (0%)]\tLoss: 0.971791\n",
      "Epoch : 92 | [1280/3600 (34%)]\tLoss: 1.005821\n",
      "Epoch : 92 | [2560/3600 (69%)]\tLoss: 0.926860\n",
      "Epoch : 93 | [0/3600 (0%)]\tLoss: 0.981575\n",
      "Epoch : 93 | [1280/3600 (34%)]\tLoss: 0.931685\n",
      "Epoch : 93 | [2560/3600 (69%)]\tLoss: 0.908548\n",
      "Epoch : 94 | [0/3600 (0%)]\tLoss: 0.927065\n",
      "Epoch : 94 | [1280/3600 (34%)]\tLoss: 0.952746\n",
      "Epoch : 94 | [2560/3600 (69%)]\tLoss: 0.940131\n",
      "Epoch : 95 | [0/3600 (0%)]\tLoss: 0.922060\n",
      "Epoch : 95 | [1280/3600 (34%)]\tLoss: 0.996446\n",
      "Epoch : 95 | [2560/3600 (69%)]\tLoss: 0.892490\n",
      "Epoch : 96 | [0/3600 (0%)]\tLoss: 0.923666\n",
      "Epoch : 96 | [1280/3600 (34%)]\tLoss: 0.905303\n",
      "Epoch : 96 | [2560/3600 (69%)]\tLoss: 0.947147\n",
      "Epoch : 97 | [0/3600 (0%)]\tLoss: 0.892739\n",
      "Epoch : 97 | [1280/3600 (34%)]\tLoss: 0.901349\n",
      "Epoch : 97 | [2560/3600 (69%)]\tLoss: 0.917954\n",
      "Epoch : 98 | [0/3600 (0%)]\tLoss: 0.937395\n",
      "Epoch : 98 | [1280/3600 (34%)]\tLoss: 0.885924\n",
      "Epoch : 98 | [2560/3600 (69%)]\tLoss: 0.928903\n",
      "Epoch : 99 | [0/3600 (0%)]\tLoss: 0.962204\n",
      "Epoch : 99 | [1280/3600 (34%)]\tLoss: 0.949064\n",
      "Epoch : 99 | [2560/3600 (69%)]\tLoss: 0.882633\n",
      "the global accuracy is 76.5%, and the global loss is 1.33.\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    model = MyModel()\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for iter in range(epoch):\n",
    "        batch_loss = []\n",
    "\n",
    "        for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            logits = model(features)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Epoch : {} | [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    iter, batch_idx * len(features),\n",
    "                    len(dataloader.dataset),\n",
    "                    100. * batch_idx / len(dataloader), loss.item()))\n",
    "            batch_loss.append(loss.item())\n",
    "        \n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    \n",
    "    test_acc, test_loss = inference(model, test_loader)\n",
    "    print('the global accuracy is {:.3}%, and the global loss is {:.3}.'.format(100 * test_acc, test_loss))\n",
    "\n",
    "train(epoch=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dirichlet分布定义及使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def dirichlet_partition(training_data, testing_data, alpha, user_num):\n",
    "    np.random.seed(2023)\n",
    "    labels_train = training_data[1]\n",
    "    labels_valid = testing_data[1]\n",
    "\n",
    "    idxs_train = np.arange(len(labels_train))\n",
    "    idxs_valid = np.arange(len(labels_valid))\n",
    "\n",
    "    idxs_labels_train = np.vstack((idxs_train, labels_train))\n",
    "    idxs_labels_train = idxs_labels_train[:, idxs_labels_train[1,:].argsort()]\n",
    "    idxs_labels_valid = np.vstack((idxs_valid, labels_valid))\n",
    "    idxs_labels_valid = idxs_labels_valid[:, idxs_labels_valid[1,:].argsort()]\n",
    "\n",
    "    labels = np.unique(labels_train, axis=0)\n",
    "\n",
    "    data_train_dict = data_organize(idxs_labels_train, labels)\n",
    "    data_valid_dict = data_organize(idxs_labels_valid, labels)\n",
    "\n",
    "    data_partition_profile_train = {}\n",
    "    data_partition_profile_valid = {}\n",
    "\n",
    "\n",
    "    for i in range(user_num):\n",
    "        data_partition_profile_train[i] = []\n",
    "        data_partition_profile_valid[i] = []\n",
    "\n",
    "    ## Distribute rest data\n",
    "    for label in data_train_dict:\n",
    "        proportions = np.random.dirichlet(np.repeat(alpha, user_num))\n",
    "        proportions_train = len(data_train_dict[label])*proportions\n",
    "        proportions_valid = len(data_valid_dict[label]) * proportions\n",
    "\n",
    "        for user in data_partition_profile_train:\n",
    "\n",
    "            data_partition_profile_train[user]   \\\n",
    "                = set.union(set(np.random.choice(data_train_dict[label], int(proportions_train[user]) , replace = False)), data_partition_profile_train[user])\n",
    "            data_train_dict[label] = list(set(data_train_dict[label])-data_partition_profile_train[user])\n",
    "\n",
    "\n",
    "            data_partition_profile_valid[user] = set.union(set(\n",
    "                np.random.choice(data_valid_dict[label], int(proportions_valid[user]),\n",
    "                                 replace=False)), data_partition_profile_valid[user])\n",
    "            data_valid_dict[label] = list(set(data_valid_dict[label]) - data_partition_profile_valid[user])\n",
    "\n",
    "\n",
    "        while len(data_train_dict[label]) != 0:\n",
    "            rest_data = data_train_dict[label][0]\n",
    "            user = np.random.randint(0, user_num)\n",
    "            data_partition_profile_train[user].add(rest_data)\n",
    "            data_train_dict[label].remove(rest_data)\n",
    "\n",
    "        while len(data_valid_dict[label]) != 0:\n",
    "            rest_data = data_valid_dict[label][0]\n",
    "            user = np.random.randint(0, user_num)\n",
    "            data_partition_profile_valid[user].add(rest_data)\n",
    "            data_valid_dict[label].remove(rest_data)\n",
    "\n",
    "    for user in data_partition_profile_train:\n",
    "        data_partition_profile_train[user] = list(data_partition_profile_train[user])\n",
    "        data_partition_profile_valid[user] = list(data_partition_profile_valid[user])\n",
    "        np.random.shuffle(data_partition_profile_train[user])\n",
    "        np.random.shuffle(data_partition_profile_valid[user])\n",
    "\n",
    "    return data_partition_profile_train, data_partition_profile_valid\n",
    "\n",
    "\n",
    "def data_organize(idxs_labels, labels):\n",
    "    data_dict = {}\n",
    "\n",
    "    labels = np.unique(labels, axis=0)\n",
    "    for one in labels:\n",
    "        data_dict[one] = []\n",
    "\n",
    "    for i in range(len(idxs_labels[1, :])):\n",
    "        data_dict[idxs_labels[1, i]].append(idxs_labels[0, i])\n",
    "    return data_dict\n",
    "\n",
    "user_num = 5\n",
    "alpha = 0.5\n",
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=alpha, user_num=user_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def visualize_partion():\n",
    "  for i in range(4):\n",
    "    y_train_part = y_train[train_index[i]]\n",
    "    label_counter = Counter(y_train_part)\n",
    "    print(f\"for part {i}\")\n",
    "    for label, count in label_counter.items():\n",
    "      print(label, \":\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for part 0\n",
      "0 : 106\n",
      "1 : 201\n",
      "3 : 119\n",
      "2 : 23\n",
      "for part 1\n",
      "1 : 393\n",
      "2 : 627\n",
      "3 : 53\n",
      "0 : 7\n",
      "for part 2\n",
      "1 : 478\n",
      "2 : 50\n",
      "for part 3\n",
      "0 : 540\n",
      "1 : 228\n",
      "3 : 130\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=0.3, user_num=user_num)\n",
    "visualize_partion()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define MyDatasetForFederal to meet the requirements of federated learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDatasetForFederal(Dataset):\n",
    "    def __init__(self, x, y, idxs):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.idxs = idxs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        xx = torch.tensor(self.x[self.idxs[index]]).to(torch.float32)\n",
    "        yy = torch.tensor(self.y[self.idxs[index]])\n",
    "        return xx, yy \n",
    "\n",
    "train_data_list = []\n",
    "for user_index in range(user_num):\n",
    "    train_data_list.append(MyDatasetForFederal(x_train, y_train, train_index[user_index]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core function for federated learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_trainer(dataset, model, global_round, device, local_epoch, batchsize, log=False):\n",
    "    dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            logits = model(features)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if log and batch_idx % 10 == 0:\n",
    "                print('| Global Round : {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    global_round, iter, batch_idx * len(features),\n",
    "                    len(dataloader.dataset),\n",
    "                    100. * batch_idx / len(dataloader), loss.item()))\n",
    "            batch_loss.append(loss.item())\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    return model.state_dict(), sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key].float(), len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 41.8%, and the global loss is 1.82.\n",
      "Global Round :1, the global accuracy is 44.0%, and the global loss is 1.76.\n",
      "Global Round :2, the global accuracy is 47.0%, and the global loss is 1.73.\n",
      "Global Round :3, the global accuracy is 50.7%, and the global loss is 1.7.\n",
      "Global Round :4, the global accuracy is 54.2%, and the global loss is 1.66.\n",
      "Global Round :5, the global accuracy is 56.8%, and the global loss is 1.62.\n",
      "Global Round :6, the global accuracy is 59.0%, and the global loss is 1.61.\n",
      "Global Round :7, the global accuracy is 59.2%, and the global loss is 1.6.\n",
      "Global Round :8, the global accuracy is 59.0%, and the global loss is 1.6.\n",
      "Global Round :9, the global accuracy is 59.2%, and the global loss is 1.52.\n",
      "Global Round :10, the global accuracy is 59.5%, and the global loss is 1.55.\n",
      "Global Round :11, the global accuracy is 59.2%, and the global loss is 1.54.\n",
      "Global Round :12, the global accuracy is 60.2%, and the global loss is 1.54.\n",
      "Global Round :13, the global accuracy is 60.0%, and the global loss is 1.52.\n",
      "Global Round :14, the global accuracy is 60.2%, and the global loss is 1.53.\n",
      "Global Round :15, the global accuracy is 60.5%, and the global loss is 1.51.\n",
      "Global Round :16, the global accuracy is 60.5%, and the global loss is 1.52.\n",
      "Global Round :17, the global accuracy is 60.5%, and the global loss is 1.5.\n",
      "Global Round :18, the global accuracy is 60.8%, and the global loss is 1.47.\n",
      "Global Round :19, the global accuracy is 60.5%, and the global loss is 1.46.\n"
     ]
    }
   ],
   "source": [
    "def federal_train_avg(log=False):\n",
    "  global_model = MyModel().to(device)\n",
    "  global_rounds = 20\n",
    "  local_epochs = 5\n",
    "  for round_idx in range(global_rounds):\n",
    "    local_weights = []\n",
    "    local_losses = []\n",
    "\n",
    "    for user_index in range(user_num):\n",
    "        model_weights, loss = local_trainer(train_data_list[user_index], copy.deepcopy(global_model), round_idx, device, local_epochs, batchsize,log)\n",
    "        local_weights.append(copy.deepcopy(model_weights))\n",
    "        local_losses.append(loss)\n",
    "\n",
    "    global_weight = average_weights(local_weights)\n",
    "    global_model.load_state_dict(global_weight)\n",
    "    test_acc, test_loss = inference(global_model, test_loader)\n",
    "\n",
    "    print('Global Round :{}, the global accuracy is {:.3}%, and the global loss is {:.3}.'.format(round_idx, 100 * test_acc, test_loss))\n",
    "  return global_model\n",
    "\n",
    "model = federal_train_avg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrate the difference in training effect of Dirichlet distribution with different alpha values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When alpha=0.1, the global accuracy is 60.8%, and the global loss is 1.53."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 49.5%, and the global loss is 1.81.\n",
      "Global Round :1, the global accuracy is 52.5%, and the global loss is 1.76.\n",
      "Global Round :2, the global accuracy is 53.8%, and the global loss is 1.72.\n",
      "Global Round :3, the global accuracy is 53.5%, and the global loss is 1.66.\n",
      "Global Round :4, the global accuracy is 54.8%, and the global loss is 1.65.\n",
      "Global Round :5, the global accuracy is 56.2%, and the global loss is 1.59.\n",
      "Global Round :6, the global accuracy is 56.5%, and the global loss is 1.61.\n",
      "Global Round :7, the global accuracy is 57.0%, and the global loss is 1.62.\n",
      "Global Round :8, the global accuracy is 57.2%, and the global loss is 1.56.\n",
      "Global Round :9, the global accuracy is 57.5%, and the global loss is 1.56.\n",
      "Global Round :10, the global accuracy is 58.0%, and the global loss is 1.56.\n",
      "Global Round :11, the global accuracy is 58.8%, and the global loss is 1.58.\n",
      "Global Round :12, the global accuracy is 59.2%, and the global loss is 1.56.\n",
      "Global Round :13, the global accuracy is 59.8%, and the global loss is 1.59.\n",
      "Global Round :14, the global accuracy is 60.2%, and the global loss is 1.56.\n",
      "Global Round :15, the global accuracy is 59.8%, and the global loss is 1.51.\n",
      "Global Round :16, the global accuracy is 60.2%, and the global loss is 1.5.\n",
      "Global Round :17, the global accuracy is 60.5%, and the global loss is 1.55.\n",
      "Global Round :18, the global accuracy is 60.5%, and the global loss is 1.53.\n",
      "Global Round :19, the global accuracy is 60.8%, and the global loss is 1.53.\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=0.1, user_num=user_num)\n",
    "train_data_list = []\n",
    "for user_index in range(user_num):\n",
    "    train_data_list.append(MyDatasetForFederal(x_train, y_train, train_index[user_index]))\n",
    "model = federal_train_avg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alpha=0.3, the global accuracy is 61.8%, and the global loss is 1.49."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 44.5%, and the global loss is 1.81.\n",
      "Global Round :1, the global accuracy is 50.0%, and the global loss is 1.76.\n",
      "Global Round :2, the global accuracy is 52.5%, and the global loss is 1.73.\n",
      "Global Round :3, the global accuracy is 53.8%, and the global loss is 1.69.\n",
      "Global Round :4, the global accuracy is 56.0%, and the global loss is 1.64.\n",
      "Global Round :5, the global accuracy is 57.5%, and the global loss is 1.65.\n",
      "Global Round :6, the global accuracy is 59.0%, and the global loss is 1.57.\n",
      "Global Round :7, the global accuracy is 59.2%, and the global loss is 1.57.\n",
      "Global Round :8, the global accuracy is 59.5%, and the global loss is 1.59.\n",
      "Global Round :9, the global accuracy is 59.8%, and the global loss is 1.54.\n",
      "Global Round :10, the global accuracy is 60.2%, and the global loss is 1.57.\n",
      "Global Round :11, the global accuracy is 60.8%, and the global loss is 1.55.\n",
      "Global Round :12, the global accuracy is 61.0%, and the global loss is 1.55.\n",
      "Global Round :13, the global accuracy is 61.0%, and the global loss is 1.53.\n",
      "Global Round :14, the global accuracy is 61.0%, and the global loss is 1.51.\n",
      "Global Round :15, the global accuracy is 61.0%, and the global loss is 1.48.\n",
      "Global Round :16, the global accuracy is 61.5%, and the global loss is 1.52.\n",
      "Global Round :17, the global accuracy is 61.5%, and the global loss is 1.47.\n",
      "Global Round :18, the global accuracy is 61.5%, and the global loss is 1.47.\n",
      "Global Round :19, the global accuracy is 61.8%, and the global loss is 1.49.\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=0.3, user_num=user_num)\n",
    "train_data_list = []\n",
    "for user_index in range(user_num):\n",
    "    train_data_list.append(MyDatasetForFederal(x_train, y_train, train_index[user_index]))\n",
    "model = federal_train_avg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alpha=0.5, the global accuracy is 72.5%, and the global loss is 1.38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 43.5%, and the global loss is 1.8.\n",
      "Global Round :1, the global accuracy is 53.8%, and the global loss is 1.76.\n",
      "Global Round :2, the global accuracy is 55.8%, and the global loss is 1.69.\n",
      "Global Round :3, the global accuracy is 56.8%, and the global loss is 1.64.\n",
      "Global Round :4, the global accuracy is 57.5%, and the global loss is 1.6.\n",
      "Global Round :5, the global accuracy is 57.8%, and the global loss is 1.54.\n",
      "Global Round :6, the global accuracy is 58.8%, and the global loss is 1.57.\n",
      "Global Round :7, the global accuracy is 59.5%, and the global loss is 1.53.\n",
      "Global Round :8, the global accuracy is 63.2%, and the global loss is 1.5.\n",
      "Global Round :9, the global accuracy is 64.8%, and the global loss is 1.48.\n",
      "Global Round :10, the global accuracy is 66.0%, and the global loss is 1.47.\n",
      "Global Round :11, the global accuracy is 66.5%, and the global loss is 1.42.\n",
      "Global Round :12, the global accuracy is 68.5%, and the global loss is 1.47.\n",
      "Global Round :13, the global accuracy is 69.2%, and the global loss is 1.45.\n",
      "Global Round :14, the global accuracy is 68.8%, and the global loss is 1.41.\n",
      "Global Round :15, the global accuracy is 70.8%, and the global loss is 1.43.\n",
      "Global Round :16, the global accuracy is 71.5%, and the global loss is 1.4.\n",
      "Global Round :17, the global accuracy is 72.0%, and the global loss is 1.36.\n",
      "Global Round :18, the global accuracy is 72.5%, and the global loss is 1.38.\n",
      "Global Round :19, the global accuracy is 72.5%, and the global loss is 1.38.\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=0.5, user_num=user_num)\n",
    "train_data_list = []\n",
    "for user_index in range(user_num):\n",
    "    train_data_list.append(MyDatasetForFederal(x_train, y_train, train_index[user_index]))\n",
    "model = federal_train_avg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alpha=1, the global accuracy is 75%, and the global loss is 1.38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 49.8%, and the global loss is 1.81.\n",
      "Global Round :1, the global accuracy is 53.8%, and the global loss is 1.74.\n",
      "Global Round :2, the global accuracy is 55.0%, and the global loss is 1.7.\n",
      "Global Round :3, the global accuracy is 56.0%, and the global loss is 1.65.\n",
      "Global Round :4, the global accuracy is 57.0%, and the global loss is 1.59.\n",
      "Global Round :5, the global accuracy is 58.5%, and the global loss is 1.53.\n",
      "Global Round :6, the global accuracy is 60.0%, and the global loss is 1.56.\n",
      "Global Round :7, the global accuracy is 62.7%, and the global loss is 1.51.\n",
      "Global Round :8, the global accuracy is 66.5%, and the global loss is 1.44.\n",
      "Global Round :9, the global accuracy is 68.8%, and the global loss is 1.51.\n",
      "Global Round :10, the global accuracy is 69.2%, and the global loss is 1.47.\n",
      "Global Round :11, the global accuracy is 69.5%, and the global loss is 1.43.\n",
      "Global Round :12, the global accuracy is 70.8%, and the global loss is 1.45.\n",
      "Global Round :13, the global accuracy is 71.8%, and the global loss is 1.4.\n",
      "Global Round :14, the global accuracy is 71.8%, and the global loss is 1.39.\n",
      "Global Round :15, the global accuracy is 72.0%, and the global loss is 1.4.\n",
      "Global Round :16, the global accuracy is 73.0%, and the global loss is 1.41.\n",
      "Global Round :17, the global accuracy is 73.0%, and the global loss is 1.38.\n",
      "Global Round :18, the global accuracy is 74.2%, and the global loss is 1.36.\n",
      "Global Round :19, the global accuracy is 75.0%, and the global loss is 1.38.\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=1, user_num=user_num)\n",
    "train_data_list = []\n",
    "for user_index in range(user_num):\n",
    "    train_data_list.append(MyDatasetForFederal(x_train, y_train, train_index[user_index]))\n",
    "model = federal_train_avg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predition = torch.argmax(model(torch.Tensor(test_features)), dim = 1)\n",
    "predition = predition + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"./Test_Data.csv\")\n",
    "df_test[\"price range\"] = predition\n",
    "df_test.to_csv(\"submission.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies to alleviate the non-iid problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The original model has an accuracy of about 61% when using alpha=0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=0.3, user_num=user_num)\n",
    "train_data_list = []\n",
    "for user_index in range(user_num):\n",
    "    train_data_list.append(MyDatasetForFederal(x_train, y_train, train_index[user_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 43.5%, and the global loss is 1.82.\n",
      "Global Round :1, the global accuracy is 51.5%, and the global loss is 1.79.\n",
      "Global Round :2, the global accuracy is 52.2%, and the global loss is 1.73.\n",
      "Global Round :3, the global accuracy is 53.5%, and the global loss is 1.69.\n",
      "Global Round :4, the global accuracy is 55.2%, and the global loss is 1.64.\n",
      "Global Round :5, the global accuracy is 57.8%, and the global loss is 1.6.\n",
      "Global Round :6, the global accuracy is 58.8%, and the global loss is 1.6.\n",
      "Global Round :7, the global accuracy is 59.5%, and the global loss is 1.58.\n",
      "Global Round :8, the global accuracy is 59.0%, and the global loss is 1.54.\n",
      "Global Round :9, the global accuracy is 59.2%, and the global loss is 1.58.\n",
      "Global Round :10, the global accuracy is 59.5%, and the global loss is 1.56.\n",
      "Global Round :11, the global accuracy is 60.0%, and the global loss is 1.54.\n",
      "Global Round :12, the global accuracy is 60.8%, and the global loss is 1.53.\n",
      "Global Round :13, the global accuracy is 60.8%, and the global loss is 1.55.\n",
      "Global Round :14, the global accuracy is 60.8%, and the global loss is 1.51.\n",
      "Global Round :15, the global accuracy is 60.8%, and the global loss is 1.54.\n",
      "Global Round :16, the global accuracy is 61.3%, and the global loss is 1.52.\n",
      "Global Round :17, the global accuracy is 61.0%, and the global loss is 1.51.\n",
      "Global Round :18, the global accuracy is 61.3%, and the global loss is 1.49.\n",
      "Global Round :19, the global accuracy is 61.8%, and the global loss is 1.45.\n"
     ]
    }
   ],
   "source": [
    "model = federal_train_avg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 1 to alleviate the non-iid problem: Add a BatchNorm1d layer. The accuracy is about 66%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型类（添加一个BatchNorm1d）\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(133, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        # 添加一个BatchNorm1d\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 50.7%, and the global loss is 1.77.\n",
      "Global Round :1, the global accuracy is 52.8%, and the global loss is 1.68.\n",
      "Global Round :2, the global accuracy is 57.0%, and the global loss is 1.62.\n",
      "Global Round :3, the global accuracy is 59.2%, and the global loss is 1.58.\n",
      "Global Round :4, the global accuracy is 60.2%, and the global loss is 1.56.\n",
      "Global Round :5, the global accuracy is 61.0%, and the global loss is 1.55.\n",
      "Global Round :6, the global accuracy is 62.3%, and the global loss is 1.55.\n",
      "Global Round :7, the global accuracy is 62.5%, and the global loss is 1.53.\n",
      "Global Round :8, the global accuracy is 62.7%, and the global loss is 1.53.\n",
      "Global Round :9, the global accuracy is 62.7%, and the global loss is 1.52.\n",
      "Global Round :10, the global accuracy is 62.5%, and the global loss is 1.5.\n",
      "Global Round :11, the global accuracy is 63.2%, and the global loss is 1.48.\n",
      "Global Round :12, the global accuracy is 63.7%, and the global loss is 1.48.\n",
      "Global Round :13, the global accuracy is 63.7%, and the global loss is 1.49.\n",
      "Global Round :14, the global accuracy is 64.2%, and the global loss is 1.45.\n",
      "Global Round :15, the global accuracy is 64.5%, and the global loss is 1.47.\n",
      "Global Round :16, the global accuracy is 65.2%, and the global loss is 1.46.\n",
      "Global Round :17, the global accuracy is 65.2%, and the global loss is 1.46.\n",
      "Global Round :18, the global accuracy is 65.5%, and the global loss is 1.43.\n",
      "Global Round :19, the global accuracy is 66.2%, and the global loss is 1.49.\n"
     ]
    }
   ],
   "source": [
    "model = federal_train_avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还原模型定义\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(133, 64)\n",
    "        self.fc2 = nn.Linear(64, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2 to alleviate the non-iid problem: Randomly select 2 clients to participate in training in each iteration. The accuracy fluctuates significantly, and the highest accuracy can reach 71%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 43.5%, and the global loss is 1.82.\n",
      "Global Round :1, the global accuracy is 40.0%, and the global loss is 1.77.\n",
      "Global Round :2, the global accuracy is 44.8%, and the global loss is 1.73.\n",
      "Global Round :3, the global accuracy is 44.0%, and the global loss is 1.68.\n",
      "Global Round :4, the global accuracy is 47.8%, and the global loss is 1.7.\n",
      "Global Round :5, the global accuracy is 60.0%, and the global loss is 1.61.\n",
      "Global Round :6, the global accuracy is 56.0%, and the global loss is 1.62.\n",
      "Global Round :7, the global accuracy is 58.5%, and the global loss is 1.56.\n",
      "Global Round :8, the global accuracy is 51.7%, and the global loss is 1.58.\n",
      "Global Round :9, the global accuracy is 58.8%, and the global loss is 1.56.\n",
      "Global Round :10, the global accuracy is 61.0%, and the global loss is 1.5.\n",
      "Global Round :11, the global accuracy is 58.2%, and the global loss is 1.53.\n",
      "Global Round :12, the global accuracy is 62.3%, and the global loss is 1.5.\n",
      "Global Round :13, the global accuracy is 69.0%, and the global loss is 1.51.\n",
      "Global Round :14, the global accuracy is 71.5%, and the global loss is 1.51.\n",
      "Global Round :15, the global accuracy is 55.5%, and the global loss is 1.59.\n",
      "Global Round :16, the global accuracy is 69.0%, and the global loss is 1.48.\n",
      "Global Round :17, the global accuracy is 58.2%, and the global loss is 1.5.\n",
      "Global Round :18, the global accuracy is 60.0%, and the global loss is 1.49.\n",
      "Global Round :19, the global accuracy is 62.3%, and the global loss is 1.46.\n",
      "Global Round :20, the global accuracy is 61.8%, and the global loss is 1.42.\n",
      "Global Round :21, the global accuracy is 68.2%, and the global loss is 1.45.\n",
      "Global Round :22, the global accuracy is 70.8%, and the global loss is 1.44.\n",
      "Global Round :23, the global accuracy is 63.0%, and the global loss is 1.49.\n",
      "Global Round :24, the global accuracy is 59.2%, and the global loss is 1.51.\n",
      "Global Round :25, the global accuracy is 65.8%, and the global loss is 1.44.\n",
      "Global Round :26, the global accuracy is 62.7%, and the global loss is 1.51.\n",
      "Global Round :27, the global accuracy is 56.0%, and the global loss is 1.55.\n",
      "Global Round :28, the global accuracy is 62.7%, and the global loss is 1.47.\n",
      "Global Round :29, the global accuracy is 60.8%, and the global loss is 1.47.\n",
      "Global Round :30, the global accuracy is 61.5%, and the global loss is 1.48.\n",
      "Global Round :31, the global accuracy is 62.3%, and the global loss is 1.48.\n",
      "Global Round :32, the global accuracy is 65.5%, and the global loss is 1.42.\n",
      "Global Round :33, the global accuracy is 72.5%, and the global loss is 1.41.\n",
      "Global Round :34, the global accuracy is 67.2%, and the global loss is 1.44.\n",
      "Global Round :35, the global accuracy is 65.5%, and the global loss is 1.41.\n",
      "Global Round :36, the global accuracy is 70.8%, and the global loss is 1.44.\n",
      "Global Round :37, the global accuracy is 60.5%, and the global loss is 1.46.\n",
      "Global Round :38, the global accuracy is 61.5%, and the global loss is 1.5.\n",
      "Global Round :39, the global accuracy is 67.5%, and the global loss is 1.39.\n",
      "Global Round :40, the global accuracy is 58.2%, and the global loss is 1.51.\n",
      "Global Round :41, the global accuracy is 62.3%, and the global loss is 1.5.\n",
      "Global Round :42, the global accuracy is 69.0%, and the global loss is 1.45.\n",
      "Global Round :43, the global accuracy is 62.3%, and the global loss is 1.44.\n",
      "Global Round :44, the global accuracy is 72.0%, and the global loss is 1.41.\n",
      "Global Round :45, the global accuracy is 64.5%, and the global loss is 1.47.\n",
      "Global Round :46, the global accuracy is 71.5%, and the global loss is 1.4.\n",
      "Global Round :47, the global accuracy is 70.0%, and the global loss is 1.49.\n",
      "Global Round :48, the global accuracy is 63.7%, and the global loss is 1.42.\n",
      "Global Round :49, the global accuracy is 62.0%, and the global loss is 1.45.\n"
     ]
    }
   ],
   "source": [
    "def federal_train_avg_sample_the_client():\n",
    "  global_model = MyModel().to(device)\n",
    "  # 因为每次只有两个客户端参与，所以rounds变成了50\n",
    "  global_rounds = 50\n",
    "  local_epochs = 5\n",
    "  for round_idx in range(global_rounds):\n",
    "      local_weights = []\n",
    "      local_losses = []\n",
    "      global_acc = []\n",
    "\n",
    "      for user_index in np.random.choice(range(user_num), size=2, replace=False):\n",
    "          model_weights, loss = local_trainer(train_data_list[user_index], copy.deepcopy(global_model), round_idx, device, local_epochs, batchsize)\n",
    "          local_weights.append(copy.deepcopy(model_weights))\n",
    "          local_losses.append(loss)\n",
    "\n",
    "      global_weight = average_weights(local_weights)\n",
    "      global_model.load_state_dict(global_weight)\n",
    "      test_acc, test_loss = inference(global_model, test_loader)\n",
    "      print('Global Round :{}, the global accuracy is {:.3}%, and the global loss is {:.3}.'.format(round_idx, 100 * test_acc, test_loss))\n",
    "\n",
    "federal_train_avg_sample_the_client()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
