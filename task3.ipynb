{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从csv中读取数据，onehot编码+归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data():\n",
    "    num_cols = [\n",
    "        \"number of rooms\",\n",
    "        \"security level of the community\",\n",
    "        \"residence space\",\n",
    "        \"noise level\",\n",
    "        \"waterfront\",\n",
    "        \"view\",\n",
    "        \"air quality level\",\n",
    "        \"aboveground space\",\n",
    "        \"building year\",\n",
    "        \"decoration year\",\n",
    "        \"is_renovated\",\n",
    "        \"lat\",\n",
    "        \"lng\",\n",
    "    ]\n",
    "    cat_cols = [\n",
    "        \"city\",\n",
    "        \"zip code\",\n",
    "    ]\n",
    "\n",
    "    feature_names = cat_cols + num_cols\n",
    "    df = pd.read_csv(\"./Train_Data_For_Task2.csv\")\n",
    "    features = df[feature_names]\n",
    "    features = pd.get_dummies(features, columns=cat_cols)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features[num_cols] = scaler.fit_transform(features[num_cols])\n",
    "\n",
    "    features = features.to_numpy()\n",
    "    labels = df[\"label\"].to_numpy()\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.1, random_state=42)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7841837959036817\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=260, max_features = 0.3)\n",
    "model.fit(x_train, y_train)\n",
    "accuracy = model.score(x_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyModel定义，满足feature->label的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (fc1): Linear(in_features=132, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义模型类\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(132, 64)\n",
    "        self.fc2 = nn.Linear(64, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel()\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyDataset定义，满足本地学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.x[index]).to(torch.float32), torch.tensor(self.y[index])\n",
    "\n",
    "batchsize = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset = MyDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.4928,  0.4356,  0.1126,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.6336, -1.5646, -1.2390,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.6480,  0.4356,  1.2204,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.4928,  0.1023, -0.7405,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4928, -0.5645, -0.8291,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.6480,  0.1023, -0.2641,  ...,  0.0000,  0.0000,  0.0000]]), tensor([1, 1, 3, 1, 2, 1, 3, 0, 1, 1, 2, 1, 0, 1, 0, 0, 2, 1, 0, 1, 1, 1, 2, 1,\n",
      "        2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 3, 0, 2, 1, 1, 0, 1, 0, 0,\n",
      "        1, 2, 3, 0, 0, 0, 2, 3, 3, 0, 3, 2, 2, 0, 0, 1, 2, 2, 0, 1, 1, 1, 0, 3,\n",
      "        1, 3, 3, 3, 1, 3, 1, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 1, 0, 1, 1, 3, 2, 0,\n",
      "        1, 3, 3, 3, 0, 1, 1, 0, 0, 1, 1, 3, 2, 1, 0, 1, 0, 3, 2, 0, 3, 1, 1, 1,\n",
      "        0, 2, 1, 0, 1, 1, 0, 1])]\n"
     ]
    }
   ],
   "source": [
    "for item in dataloader:\n",
    "  print(item)\n",
    "  break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 传统训练方式 准确率大概在76%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, testloader):\n",
    "    \"\"\" Returns the inference accuracy and loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "    for batch_idx, (features, labels) in enumerate(testloader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Inference\n",
    "        outputs = model(features)\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Prediction\n",
    "        _, pred_labels = torch.max(outputs, 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "    loss /= batch_idx\n",
    "    accuracy = correct/total\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 | [0/3600 (0%)]\tLoss: 1.382651\n",
      "Epoch : 0 | [1280/3600 (34%)]\tLoss: 1.364794\n",
      "Epoch : 0 | [2560/3600 (69%)]\tLoss: 1.355732\n",
      "Epoch : 1 | [0/3600 (0%)]\tLoss: 1.319651\n",
      "Epoch : 1 | [1280/3600 (34%)]\tLoss: 1.314043\n",
      "Epoch : 1 | [2560/3600 (69%)]\tLoss: 1.254725\n",
      "Epoch : 2 | [0/3600 (0%)]\tLoss: 1.261314\n",
      "Epoch : 2 | [1280/3600 (34%)]\tLoss: 1.246787\n",
      "Epoch : 2 | [2560/3600 (69%)]\tLoss: 1.183101\n",
      "Epoch : 3 | [0/3600 (0%)]\tLoss: 1.132513\n",
      "Epoch : 3 | [1280/3600 (34%)]\tLoss: 1.179202\n",
      "Epoch : 3 | [2560/3600 (69%)]\tLoss: 1.125111\n",
      "Epoch : 4 | [0/3600 (0%)]\tLoss: 1.177679\n",
      "Epoch : 4 | [1280/3600 (34%)]\tLoss: 1.160394\n",
      "Epoch : 4 | [2560/3600 (69%)]\tLoss: 1.161037\n",
      "Epoch : 5 | [0/3600 (0%)]\tLoss: 1.095707\n",
      "Epoch : 5 | [1280/3600 (34%)]\tLoss: 1.146005\n",
      "Epoch : 5 | [2560/3600 (69%)]\tLoss: 1.147888\n",
      "Epoch : 6 | [0/3600 (0%)]\tLoss: 1.128085\n",
      "Epoch : 6 | [1280/3600 (34%)]\tLoss: 1.086571\n",
      "Epoch : 6 | [2560/3600 (69%)]\tLoss: 1.046122\n",
      "Epoch : 7 | [0/3600 (0%)]\tLoss: 1.083811\n",
      "Epoch : 7 | [1280/3600 (34%)]\tLoss: 1.097966\n",
      "Epoch : 7 | [2560/3600 (69%)]\tLoss: 1.056579\n",
      "Epoch : 8 | [0/3600 (0%)]\tLoss: 1.095662\n",
      "Epoch : 8 | [1280/3600 (34%)]\tLoss: 1.070719\n",
      "Epoch : 8 | [2560/3600 (69%)]\tLoss: 1.074777\n",
      "Epoch : 9 | [0/3600 (0%)]\tLoss: 1.036332\n",
      "Epoch : 9 | [1280/3600 (34%)]\tLoss: 1.048156\n",
      "Epoch : 9 | [2560/3600 (69%)]\tLoss: 1.073667\n",
      "Epoch : 10 | [0/3600 (0%)]\tLoss: 1.044641\n",
      "Epoch : 10 | [1280/3600 (34%)]\tLoss: 1.043250\n",
      "Epoch : 10 | [2560/3600 (69%)]\tLoss: 1.047211\n",
      "Epoch : 11 | [0/3600 (0%)]\tLoss: 1.026276\n",
      "Epoch : 11 | [1280/3600 (34%)]\tLoss: 1.113571\n",
      "Epoch : 11 | [2560/3600 (69%)]\tLoss: 1.034811\n",
      "Epoch : 12 | [0/3600 (0%)]\tLoss: 1.021341\n",
      "Epoch : 12 | [1280/3600 (34%)]\tLoss: 1.050454\n",
      "Epoch : 12 | [2560/3600 (69%)]\tLoss: 0.993149\n",
      "Epoch : 13 | [0/3600 (0%)]\tLoss: 0.980553\n",
      "Epoch : 13 | [1280/3600 (34%)]\tLoss: 1.034877\n",
      "Epoch : 13 | [2560/3600 (69%)]\tLoss: 0.994998\n",
      "Epoch : 14 | [0/3600 (0%)]\tLoss: 1.016900\n",
      "Epoch : 14 | [1280/3600 (34%)]\tLoss: 1.000998\n",
      "Epoch : 14 | [2560/3600 (69%)]\tLoss: 1.013651\n",
      "Epoch : 15 | [0/3600 (0%)]\tLoss: 1.032549\n",
      "Epoch : 15 | [1280/3600 (34%)]\tLoss: 1.006603\n",
      "Epoch : 15 | [2560/3600 (69%)]\tLoss: 0.976021\n",
      "Epoch : 16 | [0/3600 (0%)]\tLoss: 1.020735\n",
      "Epoch : 16 | [1280/3600 (34%)]\tLoss: 1.013154\n",
      "Epoch : 16 | [2560/3600 (69%)]\tLoss: 1.005252\n",
      "Epoch : 17 | [0/3600 (0%)]\tLoss: 1.004461\n",
      "Epoch : 17 | [1280/3600 (34%)]\tLoss: 1.058213\n",
      "Epoch : 17 | [2560/3600 (69%)]\tLoss: 0.965996\n",
      "Epoch : 18 | [0/3600 (0%)]\tLoss: 1.011985\n",
      "Epoch : 18 | [1280/3600 (34%)]\tLoss: 1.081922\n",
      "Epoch : 18 | [2560/3600 (69%)]\tLoss: 1.013492\n",
      "Epoch : 19 | [0/3600 (0%)]\tLoss: 0.981038\n",
      "Epoch : 19 | [1280/3600 (34%)]\tLoss: 0.959099\n",
      "Epoch : 19 | [2560/3600 (69%)]\tLoss: 0.989196\n",
      "Epoch : 20 | [0/3600 (0%)]\tLoss: 0.958623\n",
      "Epoch : 20 | [1280/3600 (34%)]\tLoss: 1.029320\n",
      "Epoch : 20 | [2560/3600 (69%)]\tLoss: 0.953835\n",
      "Epoch : 21 | [0/3600 (0%)]\tLoss: 0.941613\n",
      "Epoch : 21 | [1280/3600 (34%)]\tLoss: 0.974437\n",
      "Epoch : 21 | [2560/3600 (69%)]\tLoss: 0.985050\n",
      "Epoch : 22 | [0/3600 (0%)]\tLoss: 0.973777\n",
      "Epoch : 22 | [1280/3600 (34%)]\tLoss: 1.000139\n",
      "Epoch : 22 | [2560/3600 (69%)]\tLoss: 1.003716\n",
      "Epoch : 23 | [0/3600 (0%)]\tLoss: 0.959300\n",
      "Epoch : 23 | [1280/3600 (34%)]\tLoss: 1.058921\n",
      "Epoch : 23 | [2560/3600 (69%)]\tLoss: 0.995636\n",
      "Epoch : 24 | [0/3600 (0%)]\tLoss: 0.976927\n",
      "Epoch : 24 | [1280/3600 (34%)]\tLoss: 0.989013\n",
      "Epoch : 24 | [2560/3600 (69%)]\tLoss: 1.022995\n",
      "Epoch : 25 | [0/3600 (0%)]\tLoss: 0.962183\n",
      "Epoch : 25 | [1280/3600 (34%)]\tLoss: 0.987270\n",
      "Epoch : 25 | [2560/3600 (69%)]\tLoss: 0.974607\n",
      "Epoch : 26 | [0/3600 (0%)]\tLoss: 1.007053\n",
      "Epoch : 26 | [1280/3600 (34%)]\tLoss: 0.965859\n",
      "Epoch : 26 | [2560/3600 (69%)]\tLoss: 0.968802\n",
      "Epoch : 27 | [0/3600 (0%)]\tLoss: 0.965373\n",
      "Epoch : 27 | [1280/3600 (34%)]\tLoss: 0.964668\n",
      "Epoch : 27 | [2560/3600 (69%)]\tLoss: 1.022457\n",
      "Epoch : 28 | [0/3600 (0%)]\tLoss: 0.966907\n",
      "Epoch : 28 | [1280/3600 (34%)]\tLoss: 0.951372\n",
      "Epoch : 28 | [2560/3600 (69%)]\tLoss: 0.972651\n",
      "Epoch : 29 | [0/3600 (0%)]\tLoss: 0.994750\n",
      "Epoch : 29 | [1280/3600 (34%)]\tLoss: 1.008713\n",
      "Epoch : 29 | [2560/3600 (69%)]\tLoss: 0.951446\n",
      "Epoch : 30 | [0/3600 (0%)]\tLoss: 0.987456\n",
      "Epoch : 30 | [1280/3600 (34%)]\tLoss: 0.967705\n",
      "Epoch : 30 | [2560/3600 (69%)]\tLoss: 1.008863\n",
      "Epoch : 31 | [0/3600 (0%)]\tLoss: 0.937051\n",
      "Epoch : 31 | [1280/3600 (34%)]\tLoss: 0.994353\n",
      "Epoch : 31 | [2560/3600 (69%)]\tLoss: 1.002442\n",
      "Epoch : 32 | [0/3600 (0%)]\tLoss: 0.990498\n",
      "Epoch : 32 | [1280/3600 (34%)]\tLoss: 0.955054\n",
      "Epoch : 32 | [2560/3600 (69%)]\tLoss: 0.938351\n",
      "Epoch : 33 | [0/3600 (0%)]\tLoss: 0.994267\n",
      "Epoch : 33 | [1280/3600 (34%)]\tLoss: 0.948401\n",
      "Epoch : 33 | [2560/3600 (69%)]\tLoss: 1.059195\n",
      "Epoch : 34 | [0/3600 (0%)]\tLoss: 0.940733\n",
      "Epoch : 34 | [1280/3600 (34%)]\tLoss: 0.955928\n",
      "Epoch : 34 | [2560/3600 (69%)]\tLoss: 1.000452\n",
      "Epoch : 35 | [0/3600 (0%)]\tLoss: 0.985714\n",
      "Epoch : 35 | [1280/3600 (34%)]\tLoss: 0.972542\n",
      "Epoch : 35 | [2560/3600 (69%)]\tLoss: 0.981935\n",
      "Epoch : 36 | [0/3600 (0%)]\tLoss: 1.001575\n",
      "Epoch : 36 | [1280/3600 (34%)]\tLoss: 0.979863\n",
      "Epoch : 36 | [2560/3600 (69%)]\tLoss: 0.944700\n",
      "Epoch : 37 | [0/3600 (0%)]\tLoss: 0.943655\n",
      "Epoch : 37 | [1280/3600 (34%)]\tLoss: 1.049163\n",
      "Epoch : 37 | [2560/3600 (69%)]\tLoss: 0.965850\n",
      "Epoch : 38 | [0/3600 (0%)]\tLoss: 0.972726\n",
      "Epoch : 38 | [1280/3600 (34%)]\tLoss: 0.992258\n",
      "Epoch : 38 | [2560/3600 (69%)]\tLoss: 0.991588\n",
      "Epoch : 39 | [0/3600 (0%)]\tLoss: 0.965099\n",
      "Epoch : 39 | [1280/3600 (34%)]\tLoss: 0.995361\n",
      "Epoch : 39 | [2560/3600 (69%)]\tLoss: 0.957681\n",
      "Epoch : 40 | [0/3600 (0%)]\tLoss: 0.988465\n",
      "Epoch : 40 | [1280/3600 (34%)]\tLoss: 0.982176\n",
      "Epoch : 40 | [2560/3600 (69%)]\tLoss: 0.963192\n",
      "Epoch : 41 | [0/3600 (0%)]\tLoss: 0.972340\n",
      "Epoch : 41 | [1280/3600 (34%)]\tLoss: 1.003796\n",
      "Epoch : 41 | [2560/3600 (69%)]\tLoss: 0.949352\n",
      "Epoch : 42 | [0/3600 (0%)]\tLoss: 0.971270\n",
      "Epoch : 42 | [1280/3600 (34%)]\tLoss: 0.929138\n",
      "Epoch : 42 | [2560/3600 (69%)]\tLoss: 0.989920\n",
      "Epoch : 43 | [0/3600 (0%)]\tLoss: 0.975985\n",
      "Epoch : 43 | [1280/3600 (34%)]\tLoss: 0.985516\n",
      "Epoch : 43 | [2560/3600 (69%)]\tLoss: 0.932833\n",
      "Epoch : 44 | [0/3600 (0%)]\tLoss: 0.952702\n",
      "Epoch : 44 | [1280/3600 (34%)]\tLoss: 0.950223\n",
      "Epoch : 44 | [2560/3600 (69%)]\tLoss: 0.992305\n",
      "Epoch : 45 | [0/3600 (0%)]\tLoss: 0.954542\n",
      "Epoch : 45 | [1280/3600 (34%)]\tLoss: 0.943951\n",
      "Epoch : 45 | [2560/3600 (69%)]\tLoss: 1.031260\n",
      "Epoch : 46 | [0/3600 (0%)]\tLoss: 0.944395\n",
      "Epoch : 46 | [1280/3600 (34%)]\tLoss: 0.953932\n",
      "Epoch : 46 | [2560/3600 (69%)]\tLoss: 0.969939\n",
      "Epoch : 47 | [0/3600 (0%)]\tLoss: 0.940238\n",
      "Epoch : 47 | [1280/3600 (34%)]\tLoss: 0.896713\n",
      "Epoch : 47 | [2560/3600 (69%)]\tLoss: 0.939998\n",
      "Epoch : 48 | [0/3600 (0%)]\tLoss: 0.937860\n",
      "Epoch : 48 | [1280/3600 (34%)]\tLoss: 1.011283\n",
      "Epoch : 48 | [2560/3600 (69%)]\tLoss: 1.034203\n",
      "Epoch : 49 | [0/3600 (0%)]\tLoss: 0.955177\n",
      "Epoch : 49 | [1280/3600 (34%)]\tLoss: 0.965531\n",
      "Epoch : 49 | [2560/3600 (69%)]\tLoss: 0.936932\n",
      "Epoch : 50 | [0/3600 (0%)]\tLoss: 0.904418\n",
      "Epoch : 50 | [1280/3600 (34%)]\tLoss: 0.938229\n",
      "Epoch : 50 | [2560/3600 (69%)]\tLoss: 0.936669\n",
      "Epoch : 51 | [0/3600 (0%)]\tLoss: 0.949200\n",
      "Epoch : 51 | [1280/3600 (34%)]\tLoss: 0.934714\n",
      "Epoch : 51 | [2560/3600 (69%)]\tLoss: 0.928946\n",
      "Epoch : 52 | [0/3600 (0%)]\tLoss: 0.912995\n",
      "Epoch : 52 | [1280/3600 (34%)]\tLoss: 0.982710\n",
      "Epoch : 52 | [2560/3600 (69%)]\tLoss: 0.940701\n",
      "Epoch : 53 | [0/3600 (0%)]\tLoss: 0.861732\n",
      "Epoch : 53 | [1280/3600 (34%)]\tLoss: 0.963635\n",
      "Epoch : 53 | [2560/3600 (69%)]\tLoss: 0.971180\n",
      "Epoch : 54 | [0/3600 (0%)]\tLoss: 1.000975\n",
      "Epoch : 54 | [1280/3600 (34%)]\tLoss: 0.931202\n",
      "Epoch : 54 | [2560/3600 (69%)]\tLoss: 0.955814\n",
      "Epoch : 55 | [0/3600 (0%)]\tLoss: 0.983435\n",
      "Epoch : 55 | [1280/3600 (34%)]\tLoss: 0.954214\n",
      "Epoch : 55 | [2560/3600 (69%)]\tLoss: 0.934176\n",
      "Epoch : 56 | [0/3600 (0%)]\tLoss: 0.958179\n",
      "Epoch : 56 | [1280/3600 (34%)]\tLoss: 0.895380\n",
      "Epoch : 56 | [2560/3600 (69%)]\tLoss: 0.916593\n",
      "Epoch : 57 | [0/3600 (0%)]\tLoss: 0.948609\n",
      "Epoch : 57 | [1280/3600 (34%)]\tLoss: 1.002526\n",
      "Epoch : 57 | [2560/3600 (69%)]\tLoss: 0.923389\n",
      "Epoch : 58 | [0/3600 (0%)]\tLoss: 0.931414\n",
      "Epoch : 58 | [1280/3600 (34%)]\tLoss: 0.889232\n",
      "Epoch : 58 | [2560/3600 (69%)]\tLoss: 0.914450\n",
      "Epoch : 59 | [0/3600 (0%)]\tLoss: 0.914728\n",
      "Epoch : 59 | [1280/3600 (34%)]\tLoss: 0.979204\n",
      "Epoch : 59 | [2560/3600 (69%)]\tLoss: 0.971461\n",
      "Epoch : 60 | [0/3600 (0%)]\tLoss: 0.918960\n",
      "Epoch : 60 | [1280/3600 (34%)]\tLoss: 0.953191\n",
      "Epoch : 60 | [2560/3600 (69%)]\tLoss: 0.991938\n",
      "Epoch : 61 | [0/3600 (0%)]\tLoss: 0.988699\n",
      "Epoch : 61 | [1280/3600 (34%)]\tLoss: 0.941379\n",
      "Epoch : 61 | [2560/3600 (69%)]\tLoss: 0.947226\n",
      "Epoch : 62 | [0/3600 (0%)]\tLoss: 0.943206\n",
      "Epoch : 62 | [1280/3600 (34%)]\tLoss: 0.956159\n",
      "Epoch : 62 | [2560/3600 (69%)]\tLoss: 0.912960\n",
      "Epoch : 63 | [0/3600 (0%)]\tLoss: 0.894601\n",
      "Epoch : 63 | [1280/3600 (34%)]\tLoss: 0.953713\n",
      "Epoch : 63 | [2560/3600 (69%)]\tLoss: 0.977344\n",
      "Epoch : 64 | [0/3600 (0%)]\tLoss: 0.934434\n",
      "Epoch : 64 | [1280/3600 (34%)]\tLoss: 0.917415\n",
      "Epoch : 64 | [2560/3600 (69%)]\tLoss: 0.937631\n",
      "Epoch : 65 | [0/3600 (0%)]\tLoss: 0.967344\n",
      "Epoch : 65 | [1280/3600 (34%)]\tLoss: 0.945979\n",
      "Epoch : 65 | [2560/3600 (69%)]\tLoss: 0.947948\n",
      "Epoch : 66 | [0/3600 (0%)]\tLoss: 0.932095\n",
      "Epoch : 66 | [1280/3600 (34%)]\tLoss: 0.988383\n",
      "Epoch : 66 | [2560/3600 (69%)]\tLoss: 0.998959\n",
      "Epoch : 67 | [0/3600 (0%)]\tLoss: 0.903008\n",
      "Epoch : 67 | [1280/3600 (34%)]\tLoss: 0.955266\n",
      "Epoch : 67 | [2560/3600 (69%)]\tLoss: 0.922923\n",
      "Epoch : 68 | [0/3600 (0%)]\tLoss: 0.921070\n",
      "Epoch : 68 | [1280/3600 (34%)]\tLoss: 0.952406\n",
      "Epoch : 68 | [2560/3600 (69%)]\tLoss: 0.981831\n",
      "Epoch : 69 | [0/3600 (0%)]\tLoss: 0.942509\n",
      "Epoch : 69 | [1280/3600 (34%)]\tLoss: 0.959626\n",
      "Epoch : 69 | [2560/3600 (69%)]\tLoss: 0.931482\n",
      "Epoch : 70 | [0/3600 (0%)]\tLoss: 0.910189\n",
      "Epoch : 70 | [1280/3600 (34%)]\tLoss: 0.887962\n",
      "Epoch : 70 | [2560/3600 (69%)]\tLoss: 0.944488\n",
      "Epoch : 71 | [0/3600 (0%)]\tLoss: 0.890900\n",
      "Epoch : 71 | [1280/3600 (34%)]\tLoss: 0.932223\n",
      "Epoch : 71 | [2560/3600 (69%)]\tLoss: 0.893012\n",
      "Epoch : 72 | [0/3600 (0%)]\tLoss: 0.903448\n",
      "Epoch : 72 | [1280/3600 (34%)]\tLoss: 0.924153\n",
      "Epoch : 72 | [2560/3600 (69%)]\tLoss: 0.930886\n",
      "Epoch : 73 | [0/3600 (0%)]\tLoss: 0.962207\n",
      "Epoch : 73 | [1280/3600 (34%)]\tLoss: 0.976737\n",
      "Epoch : 73 | [2560/3600 (69%)]\tLoss: 0.955437\n",
      "Epoch : 74 | [0/3600 (0%)]\tLoss: 0.965982\n",
      "Epoch : 74 | [1280/3600 (34%)]\tLoss: 0.923863\n",
      "Epoch : 74 | [2560/3600 (69%)]\tLoss: 1.011575\n",
      "Epoch : 75 | [0/3600 (0%)]\tLoss: 0.960829\n",
      "Epoch : 75 | [1280/3600 (34%)]\tLoss: 0.931589\n",
      "Epoch : 75 | [2560/3600 (69%)]\tLoss: 0.926136\n",
      "Epoch : 76 | [0/3600 (0%)]\tLoss: 0.932421\n",
      "Epoch : 76 | [1280/3600 (34%)]\tLoss: 0.934799\n",
      "Epoch : 76 | [2560/3600 (69%)]\tLoss: 0.918863\n",
      "Epoch : 77 | [0/3600 (0%)]\tLoss: 0.952308\n",
      "Epoch : 77 | [1280/3600 (34%)]\tLoss: 0.921500\n",
      "Epoch : 77 | [2560/3600 (69%)]\tLoss: 0.914840\n",
      "Epoch : 78 | [0/3600 (0%)]\tLoss: 0.905358\n",
      "Epoch : 78 | [1280/3600 (34%)]\tLoss: 0.954704\n",
      "Epoch : 78 | [2560/3600 (69%)]\tLoss: 0.933717\n",
      "Epoch : 79 | [0/3600 (0%)]\tLoss: 0.939879\n",
      "Epoch : 79 | [1280/3600 (34%)]\tLoss: 0.914034\n",
      "Epoch : 79 | [2560/3600 (69%)]\tLoss: 0.944002\n",
      "Epoch : 80 | [0/3600 (0%)]\tLoss: 0.917640\n",
      "Epoch : 80 | [1280/3600 (34%)]\tLoss: 0.952269\n",
      "Epoch : 80 | [2560/3600 (69%)]\tLoss: 0.930556\n",
      "Epoch : 81 | [0/3600 (0%)]\tLoss: 0.924142\n",
      "Epoch : 81 | [1280/3600 (34%)]\tLoss: 0.935229\n",
      "Epoch : 81 | [2560/3600 (69%)]\tLoss: 0.907353\n",
      "Epoch : 82 | [0/3600 (0%)]\tLoss: 0.898559\n",
      "Epoch : 82 | [1280/3600 (34%)]\tLoss: 0.917468\n",
      "Epoch : 82 | [2560/3600 (69%)]\tLoss: 0.962341\n",
      "Epoch : 83 | [0/3600 (0%)]\tLoss: 0.961592\n",
      "Epoch : 83 | [1280/3600 (34%)]\tLoss: 0.890215\n",
      "Epoch : 83 | [2560/3600 (69%)]\tLoss: 0.938137\n",
      "Epoch : 84 | [0/3600 (0%)]\tLoss: 0.915167\n",
      "Epoch : 84 | [1280/3600 (34%)]\tLoss: 0.942492\n",
      "Epoch : 84 | [2560/3600 (69%)]\tLoss: 0.964692\n",
      "Epoch : 85 | [0/3600 (0%)]\tLoss: 0.932304\n",
      "Epoch : 85 | [1280/3600 (34%)]\tLoss: 0.927054\n",
      "Epoch : 85 | [2560/3600 (69%)]\tLoss: 0.936680\n",
      "Epoch : 86 | [0/3600 (0%)]\tLoss: 0.905668\n",
      "Epoch : 86 | [1280/3600 (34%)]\tLoss: 0.976761\n",
      "Epoch : 86 | [2560/3600 (69%)]\tLoss: 0.897992\n",
      "Epoch : 87 | [0/3600 (0%)]\tLoss: 0.920867\n",
      "Epoch : 87 | [1280/3600 (34%)]\tLoss: 0.880400\n",
      "Epoch : 87 | [2560/3600 (69%)]\tLoss: 0.907814\n",
      "Epoch : 88 | [0/3600 (0%)]\tLoss: 0.931486\n",
      "Epoch : 88 | [1280/3600 (34%)]\tLoss: 0.907362\n",
      "Epoch : 88 | [2560/3600 (69%)]\tLoss: 0.915476\n",
      "Epoch : 89 | [0/3600 (0%)]\tLoss: 0.947233\n",
      "Epoch : 89 | [1280/3600 (34%)]\tLoss: 0.910962\n",
      "Epoch : 89 | [2560/3600 (69%)]\tLoss: 0.945566\n",
      "Epoch : 90 | [0/3600 (0%)]\tLoss: 0.924616\n",
      "Epoch : 90 | [1280/3600 (34%)]\tLoss: 0.903582\n",
      "Epoch : 90 | [2560/3600 (69%)]\tLoss: 0.933010\n",
      "Epoch : 91 | [0/3600 (0%)]\tLoss: 0.922391\n",
      "Epoch : 91 | [1280/3600 (34%)]\tLoss: 0.934921\n",
      "Epoch : 91 | [2560/3600 (69%)]\tLoss: 0.905092\n",
      "Epoch : 92 | [0/3600 (0%)]\tLoss: 0.878512\n",
      "Epoch : 92 | [1280/3600 (34%)]\tLoss: 0.960467\n",
      "Epoch : 92 | [2560/3600 (69%)]\tLoss: 0.923700\n",
      "Epoch : 93 | [0/3600 (0%)]\tLoss: 0.930161\n",
      "Epoch : 93 | [1280/3600 (34%)]\tLoss: 0.942759\n",
      "Epoch : 93 | [2560/3600 (69%)]\tLoss: 0.899433\n",
      "Epoch : 94 | [0/3600 (0%)]\tLoss: 0.889463\n",
      "Epoch : 94 | [1280/3600 (34%)]\tLoss: 0.950586\n",
      "Epoch : 94 | [2560/3600 (69%)]\tLoss: 0.865409\n",
      "Epoch : 95 | [0/3600 (0%)]\tLoss: 0.923132\n",
      "Epoch : 95 | [1280/3600 (34%)]\tLoss: 0.973571\n",
      "Epoch : 95 | [2560/3600 (69%)]\tLoss: 0.881051\n",
      "Epoch : 96 | [0/3600 (0%)]\tLoss: 0.954795\n",
      "Epoch : 96 | [1280/3600 (34%)]\tLoss: 0.912347\n",
      "Epoch : 96 | [2560/3600 (69%)]\tLoss: 0.981289\n",
      "Epoch : 97 | [0/3600 (0%)]\tLoss: 0.945864\n",
      "Epoch : 97 | [1280/3600 (34%)]\tLoss: 0.924650\n",
      "Epoch : 97 | [2560/3600 (69%)]\tLoss: 0.912425\n",
      "Epoch : 98 | [0/3600 (0%)]\tLoss: 0.933651\n",
      "Epoch : 98 | [1280/3600 (34%)]\tLoss: 0.899519\n",
      "Epoch : 98 | [2560/3600 (69%)]\tLoss: 0.947131\n",
      "Epoch : 99 | [0/3600 (0%)]\tLoss: 0.860013\n",
      "Epoch : 99 | [1280/3600 (34%)]\tLoss: 0.937440\n",
      "Epoch : 99 | [2560/3600 (69%)]\tLoss: 0.940302\n",
      "the global accuracy is 75.5%, and the global loss is 1.29.\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    model = MyModel()\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    epoch_test_loss = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for iter in range(epoch):\n",
    "        batch_loss = []\n",
    "\n",
    "        for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            logits = model(features)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Epoch : {} | [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    iter, batch_idx * len(features),\n",
    "                    len(dataloader.dataset),\n",
    "                    100. * batch_idx / len(dataloader), loss.item()))\n",
    "            batch_loss.append(loss.item())\n",
    "        \n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    \n",
    "    test_acc, test_loss = inference(model, test_loader)\n",
    "    print('the global accuracy is {:.3}%, and the global loss is {:.3}.'.format(100 * test_acc, test_loss))\n",
    "\n",
    "train(epoch=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dirichlet分布定义及使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def dirichlet_partition(training_data, testing_data, alpha, user_num):\n",
    "    np.random.seed(2023)\n",
    "    labels_train = training_data[1]\n",
    "    labels_valid = testing_data[1]\n",
    "\n",
    "    idxs_train = np.arange(len(labels_train))\n",
    "    idxs_valid = np.arange(len(labels_valid))\n",
    "\n",
    "    idxs_labels_train = np.vstack((idxs_train, labels_train))\n",
    "    # print(\"idxs_labels_train 1: \", idxs_labels_train)\n",
    "    idxs_labels_train = idxs_labels_train[:, idxs_labels_train[1,:].argsort()]\n",
    "    # print(\"idxs_labels_train 2: \", idxs_labels_train)\n",
    "    idxs_labels_valid = np.vstack((idxs_valid, labels_valid))\n",
    "    idxs_labels_valid = idxs_labels_valid[:, idxs_labels_valid[1,:].argsort()]\n",
    "\n",
    "    labels = np.unique(labels_train, axis=0)\n",
    "\n",
    "    data_train_dict = data_organize(idxs_labels_train, labels)\n",
    "    # print(\"data_train_dict[0]: \", data_train_dict[0])\n",
    "    # print(\"data_train_dict[9]: \", data_train_dict[9])\n",
    "    data_valid_dict = data_organize(idxs_labels_valid, labels)\n",
    "\n",
    "    data_partition_profile_train = {}\n",
    "    data_partition_profile_valid = {}\n",
    "\n",
    "\n",
    "    for i in range(user_num):\n",
    "        data_partition_profile_train[i] = []\n",
    "        data_partition_profile_valid[i] = []\n",
    "\n",
    "    ## Distribute rest data\n",
    "    for label in data_train_dict:\n",
    "        proportions = np.random.dirichlet(np.repeat(alpha, user_num))\n",
    "        proportions_train = len(data_train_dict[label])*proportions\n",
    "        proportions_valid = len(data_valid_dict[label]) * proportions\n",
    "\n",
    "        for user in data_partition_profile_train:\n",
    "\n",
    "            data_partition_profile_train[user]   \\\n",
    "                = set.union(set(np.random.choice(data_train_dict[label], int(proportions_train[user]) , replace = False)), data_partition_profile_train[user])\n",
    "            data_train_dict[label] = list(set(data_train_dict[label])-data_partition_profile_train[user])\n",
    "\n",
    "\n",
    "            data_partition_profile_valid[user] = set.union(set(\n",
    "                np.random.choice(data_valid_dict[label], int(proportions_valid[user]),\n",
    "                                 replace=False)), data_partition_profile_valid[user])\n",
    "            data_valid_dict[label] = list(set(data_valid_dict[label]) - data_partition_profile_valid[user])\n",
    "\n",
    "\n",
    "        while len(data_train_dict[label]) != 0:\n",
    "            rest_data = data_train_dict[label][0]\n",
    "            user = np.random.randint(0, user_num)\n",
    "            data_partition_profile_train[user].add(rest_data)\n",
    "            data_train_dict[label].remove(rest_data)\n",
    "\n",
    "        while len(data_valid_dict[label]) != 0:\n",
    "            rest_data = data_valid_dict[label][0]\n",
    "            user = np.random.randint(0, user_num)\n",
    "            data_partition_profile_valid[user].add(rest_data)\n",
    "            data_valid_dict[label].remove(rest_data)\n",
    "\n",
    "    for user in data_partition_profile_train:\n",
    "        data_partition_profile_train[user] = list(data_partition_profile_train[user])\n",
    "        data_partition_profile_valid[user] = list(data_partition_profile_valid[user])\n",
    "        np.random.shuffle(data_partition_profile_train[user])\n",
    "        np.random.shuffle(data_partition_profile_valid[user])\n",
    "\n",
    "    return data_partition_profile_train, data_partition_profile_valid\n",
    "\n",
    "\n",
    "def data_organize(idxs_labels, labels):\n",
    "    data_dict = {}\n",
    "\n",
    "    labels = np.unique(labels, axis=0)\n",
    "    for one in labels:\n",
    "        data_dict[one] = []\n",
    "\n",
    "    for i in range(len(idxs_labels[1, :])):\n",
    "        data_dict[idxs_labels[1, i]].append(idxs_labels[0, i])\n",
    "    return data_dict\n",
    "\n",
    "user_num = 5\n",
    "alpha = 0.5\n",
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=alpha, user_num=user_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def visualize_partion():\n",
    "  for i in range(4):\n",
    "    y_train_part = y_train[train_index[i]]\n",
    "    label_counter = Counter(y_train_part)\n",
    "    print(f\"for part {i}\")\n",
    "    for label, count in label_counter.items():\n",
    "      print(label, \":\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for part 0\n",
      "0 : 106\n",
      "1 : 201\n",
      "3 : 119\n",
      "2 : 23\n",
      "for part 1\n",
      "1 : 393\n",
      "2 : 627\n",
      "3 : 53\n",
      "0 : 7\n",
      "for part 2\n",
      "1 : 478\n",
      "2 : 50\n",
      "for part 3\n",
      "0 : 540\n",
      "1 : 228\n",
      "3 : 130\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=0.3, user_num=user_num)\n",
    "visualize_partion()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyDatasetForFederal定义，满足联邦学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDatasetForFederal(Dataset):\n",
    "    def __init__(self, x, y, idxs):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.idxs = idxs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        xx = torch.tensor(self.x[self.idxs[index]]).to(torch.float32)\n",
    "        yy = torch.tensor(self.y[self.idxs[index]])\n",
    "        return xx, yy \n",
    "\n",
    "train_data_list = []\n",
    "for user_index in range(user_num):\n",
    "    train_data_list.append(MyDatasetForFederal(x_train, y_train, train_index[user_index]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 联邦学习核心函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_trainer(dataset, model, global_round, device, local_epoch, batchsize, log=False):\n",
    "    dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            logits = model(features)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if log and batch_idx % 10 == 0:\n",
    "                print('| Global Round : {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    global_round, iter, batch_idx * len(features),\n",
    "                    len(dataloader.dataset),\n",
    "                    100. * batch_idx / len(dataloader), loss.item()))\n",
    "            batch_loss.append(loss.item())\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    return model.state_dict(), sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key].float(), len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 42.0%, and the global loss is 1.81.\n",
      "Global Round :1, the global accuracy is 47.8%, and the global loss is 1.76.\n",
      "Global Round :2, the global accuracy is 52.2%, and the global loss is 1.72.\n",
      "Global Round :3, the global accuracy is 54.8%, and the global loss is 1.71.\n",
      "Global Round :4, the global accuracy is 56.2%, and the global loss is 1.65.\n",
      "Global Round :5, the global accuracy is 57.2%, and the global loss is 1.63.\n",
      "Global Round :6, the global accuracy is 58.8%, and the global loss is 1.62.\n",
      "Global Round :7, the global accuracy is 59.8%, and the global loss is 1.53.\n",
      "Global Round :8, the global accuracy is 60.5%, and the global loss is 1.54.\n",
      "Global Round :9, the global accuracy is 60.0%, and the global loss is 1.59.\n",
      "Global Round :10, the global accuracy is 60.0%, and the global loss is 1.52.\n",
      "Global Round :11, the global accuracy is 60.5%, and the global loss is 1.56.\n",
      "Global Round :12, the global accuracy is 60.5%, and the global loss is 1.53.\n",
      "Global Round :13, the global accuracy is 60.5%, and the global loss is 1.53.\n",
      "Global Round :14, the global accuracy is 59.8%, and the global loss is 1.52.\n",
      "Global Round :15, the global accuracy is 60.8%, and the global loss is 1.51.\n",
      "Global Round :16, the global accuracy is 60.8%, and the global loss is 1.52.\n",
      "Global Round :17, the global accuracy is 60.8%, and the global loss is 1.51.\n",
      "Global Round :18, the global accuracy is 60.8%, and the global loss is 1.49.\n",
      "Global Round :19, the global accuracy is 60.5%, and the global loss is 1.54.\n"
     ]
    }
   ],
   "source": [
    "def federal_train_avg(log=False):\n",
    "  global_model = MyModel().to(device)\n",
    "  global_rounds = 20\n",
    "  local_epochs = 5\n",
    "  for round_idx in range(global_rounds):\n",
    "    local_weights = []\n",
    "    local_losses = []\n",
    "    global_acc = []\n",
    "\n",
    "    for user_index in range(user_num):\n",
    "        model_weights, loss = local_trainer(train_data_list[user_index], copy.deepcopy(global_model), round_idx, device, local_epochs, batchsize,log)\n",
    "        local_weights.append(copy.deepcopy(model_weights))\n",
    "        local_losses.append(loss)\n",
    "\n",
    "    global_weight = average_weights(local_weights)\n",
    "    global_model.load_state_dict(global_weight)\n",
    "    test_acc, test_loss = inference(global_model, test_loader)\n",
    "\n",
    "    print('Global Round :{}, the global accuracy is {:.3}%, and the global loss is {:.3}.'.format(round_idx, 100 * test_acc, test_loss))\n",
    "\n",
    "federal_train_avg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演示dirichlet分布不同alpha值，训练效果的差别"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alpha=0.1, the global accuracy is 59.0%, and the global loss is 1.53."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 45.0%, and the global loss is 1.82.\n",
      "Global Round :1, the global accuracy is 51.0%, and the global loss is 1.77.\n",
      "Global Round :2, the global accuracy is 53.0%, and the global loss is 1.72.\n",
      "Global Round :3, the global accuracy is 54.2%, and the global loss is 1.66.\n",
      "Global Round :4, the global accuracy is 55.2%, and the global loss is 1.66.\n",
      "Global Round :5, the global accuracy is 56.2%, and the global loss is 1.64.\n",
      "Global Round :6, the global accuracy is 57.5%, and the global loss is 1.59.\n",
      "Global Round :7, the global accuracy is 57.8%, and the global loss is 1.58.\n",
      "Global Round :8, the global accuracy is 58.2%, and the global loss is 1.59.\n",
      "Global Round :9, the global accuracy is 58.8%, and the global loss is 1.54.\n",
      "Global Round :10, the global accuracy is 59.0%, and the global loss is 1.57.\n",
      "Global Round :11, the global accuracy is 59.0%, and the global loss is 1.56.\n",
      "Global Round :12, the global accuracy is 59.2%, and the global loss is 1.55.\n",
      "Global Round :13, the global accuracy is 59.0%, and the global loss is 1.57.\n",
      "Global Round :14, the global accuracy is 59.2%, and the global loss is 1.56.\n",
      "Global Round :15, the global accuracy is 59.8%, and the global loss is 1.54.\n",
      "Global Round :16, the global accuracy is 59.8%, and the global loss is 1.48.\n",
      "Global Round :17, the global accuracy is 59.2%, and the global loss is 1.53.\n",
      "Global Round :18, the global accuracy is 59.2%, and the global loss is 1.52.\n",
      "Global Round :19, the global accuracy is 59.8%, and the global loss is 1.53.\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=0.1, user_num=user_num)\n",
    "train_data_list = []\n",
    "for user_index in range(user_num):\n",
    "    train_data_list.append(MyDatasetForFederal(x_train, y_train, train_index[user_index]))\n",
    "federal_train_avg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alpha=0.3, the global accuracy is 61.8%, and the global loss is 1.47."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 46.8%, and the global loss is 1.82.\n",
      "Global Round :1, the global accuracy is 51.2%, and the global loss is 1.77.\n",
      "Global Round :2, the global accuracy is 53.8%, and the global loss is 1.71.\n",
      "Global Round :3, the global accuracy is 56.0%, and the global loss is 1.68.\n",
      "Global Round :4, the global accuracy is 56.5%, and the global loss is 1.66.\n",
      "Global Round :5, the global accuracy is 57.5%, and the global loss is 1.63.\n",
      "Global Round :6, the global accuracy is 59.0%, and the global loss is 1.59.\n",
      "Global Round :7, the global accuracy is 58.8%, and the global loss is 1.57.\n",
      "Global Round :8, the global accuracy is 59.0%, and the global loss is 1.58.\n",
      "Global Round :9, the global accuracy is 59.8%, and the global loss is 1.55.\n",
      "Global Round :10, the global accuracy is 59.8%, and the global loss is 1.54.\n",
      "Global Round :11, the global accuracy is 60.0%, and the global loss is 1.49.\n",
      "Global Round :12, the global accuracy is 60.2%, and the global loss is 1.54.\n",
      "Global Round :13, the global accuracy is 60.2%, and the global loss is 1.52.\n",
      "Global Round :14, the global accuracy is 60.8%, and the global loss is 1.51.\n",
      "Global Round :15, the global accuracy is 61.3%, and the global loss is 1.52.\n",
      "Global Round :16, the global accuracy is 61.0%, and the global loss is 1.5.\n",
      "Global Round :17, the global accuracy is 61.5%, and the global loss is 1.5.\n",
      "Global Round :18, the global accuracy is 61.8%, and the global loss is 1.49.\n",
      "Global Round :19, the global accuracy is 61.8%, and the global loss is 1.47.\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=0.3, user_num=user_num)\n",
    "train_data_list = []\n",
    "for user_index in range(user_num):\n",
    "    train_data_list.append(MyDatasetForFederal(x_train, y_train, train_index[user_index]))\n",
    "federal_train_avg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alpha=0.5, the global accuracy is 73.0%, and the global loss is 1.39."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 53.0%, and the global loss is 1.79.\n",
      "Global Round :1, the global accuracy is 53.2%, and the global loss is 1.74.\n",
      "Global Round :2, the global accuracy is 53.5%, and the global loss is 1.7.\n",
      "Global Round :3, the global accuracy is 56.0%, and the global loss is 1.63.\n",
      "Global Round :4, the global accuracy is 57.2%, and the global loss is 1.62.\n",
      "Global Round :5, the global accuracy is 58.2%, and the global loss is 1.54.\n",
      "Global Round :6, the global accuracy is 59.2%, and the global loss is 1.54.\n",
      "Global Round :7, the global accuracy is 60.8%, and the global loss is 1.5.\n",
      "Global Round :8, the global accuracy is 62.3%, and the global loss is 1.49.\n",
      "Global Round :9, the global accuracy is 64.0%, and the global loss is 1.5.\n",
      "Global Round :10, the global accuracy is 65.2%, and the global loss is 1.46.\n",
      "Global Round :11, the global accuracy is 67.2%, and the global loss is 1.46.\n",
      "Global Round :12, the global accuracy is 68.8%, and the global loss is 1.44.\n",
      "Global Round :13, the global accuracy is 69.2%, and the global loss is 1.44.\n",
      "Global Round :14, the global accuracy is 69.8%, and the global loss is 1.43.\n",
      "Global Round :15, the global accuracy is 70.2%, and the global loss is 1.47.\n",
      "Global Round :16, the global accuracy is 71.8%, and the global loss is 1.39.\n",
      "Global Round :17, the global accuracy is 72.5%, and the global loss is 1.41.\n",
      "Global Round :18, the global accuracy is 72.5%, and the global loss is 1.38.\n",
      "Global Round :19, the global accuracy is 73.0%, and the global loss is 1.39.\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=0.5, user_num=user_num)\n",
    "train_data_list = []\n",
    "for user_index in range(user_num):\n",
    "    train_data_list.append(MyDatasetForFederal(x_train, y_train, train_index[user_index]))\n",
    "federal_train_avg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alpha=1, the global accuracy is 74.5%, and the global loss is 1.35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 55.0%, and the global loss is 1.8.\n",
      "Global Round :1, the global accuracy is 57.0%, and the global loss is 1.75.\n",
      "Global Round :2, the global accuracy is 56.5%, and the global loss is 1.66.\n",
      "Global Round :3, the global accuracy is 57.5%, and the global loss is 1.64.\n",
      "Global Round :4, the global accuracy is 60.8%, and the global loss is 1.57.\n",
      "Global Round :5, the global accuracy is 63.2%, and the global loss is 1.54.\n",
      "Global Round :6, the global accuracy is 65.2%, and the global loss is 1.56.\n",
      "Global Round :7, the global accuracy is 67.0%, and the global loss is 1.55.\n",
      "Global Round :8, the global accuracy is 67.5%, and the global loss is 1.49.\n",
      "Global Round :9, the global accuracy is 68.8%, and the global loss is 1.46.\n",
      "Global Round :10, the global accuracy is 68.5%, and the global loss is 1.42.\n",
      "Global Round :11, the global accuracy is 69.8%, and the global loss is 1.4.\n",
      "Global Round :12, the global accuracy is 71.2%, and the global loss is 1.5.\n",
      "Global Round :13, the global accuracy is 72.0%, and the global loss is 1.42.\n",
      "Global Round :14, the global accuracy is 72.2%, and the global loss is 1.42.\n",
      "Global Round :15, the global accuracy is 73.5%, and the global loss is 1.4.\n",
      "Global Round :16, the global accuracy is 73.5%, and the global loss is 1.34.\n",
      "Global Round :17, the global accuracy is 73.0%, and the global loss is 1.39.\n",
      "Global Round :18, the global accuracy is 74.2%, and the global loss is 1.37.\n",
      "Global Round :19, the global accuracy is 74.5%, and the global loss is 1.35.\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=1, user_num=user_num)\n",
    "train_data_list = []\n",
    "for user_index in range(user_num):\n",
    "    train_data_list.append(MyDatasetForFederal(x_train, y_train, train_index[user_index]))\n",
    "federal_train_avg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 缓解方案"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原来模型, 使用alpha=0.3, 准确率大概60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, test_index = dirichlet_partition((x_train, y_train), (x_test, y_test), alpha=0.3, user_num=user_num)\n",
    "train_data_list = []\n",
    "for user_index in range(user_num):\n",
    "    train_data_list.append(MyDatasetForFederal(x_train, y_train, train_index[user_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 43.8%, and the global loss is 1.81.\n",
      "Global Round :1, the global accuracy is 49.2%, and the global loss is 1.79.\n",
      "Global Round :2, the global accuracy is 52.0%, and the global loss is 1.74.\n",
      "Global Round :3, the global accuracy is 53.8%, and the global loss is 1.69.\n",
      "Global Round :4, the global accuracy is 56.5%, and the global loss is 1.64.\n",
      "Global Round :5, the global accuracy is 57.8%, and the global loss is 1.63.\n",
      "Global Round :6, the global accuracy is 58.2%, and the global loss is 1.6.\n",
      "Global Round :7, the global accuracy is 58.2%, and the global loss is 1.58.\n",
      "Global Round :8, the global accuracy is 59.2%, and the global loss is 1.62.\n",
      "Global Round :9, the global accuracy is 59.2%, and the global loss is 1.58.\n",
      "Global Round :10, the global accuracy is 59.5%, and the global loss is 1.58.\n",
      "Global Round :11, the global accuracy is 59.8%, and the global loss is 1.54.\n",
      "Global Round :12, the global accuracy is 60.0%, and the global loss is 1.53.\n",
      "Global Round :13, the global accuracy is 60.2%, and the global loss is 1.5.\n",
      "Global Round :14, the global accuracy is 60.0%, and the global loss is 1.51.\n",
      "Global Round :15, the global accuracy is 60.2%, and the global loss is 1.54.\n",
      "Global Round :16, the global accuracy is 60.5%, and the global loss is 1.48.\n",
      "Global Round :17, the global accuracy is 60.8%, and the global loss is 1.54.\n",
      "Global Round :18, the global accuracy is 61.3%, and the global loss is 1.5.\n",
      "Global Round :19, the global accuracy is 61.0%, and the global loss is 1.48.\n"
     ]
    }
   ],
   "source": [
    "federal_train_avg()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缓解方案1： 添加一个BatchNorm1d, 准确率大概67%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型类（添加一个BatchNorm1d）\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(132, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        # 添加一个BatchNorm1d\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 52.2%, and the global loss is 1.77.\n",
      "Global Round :1, the global accuracy is 57.8%, and the global loss is 1.67.\n",
      "Global Round :2, the global accuracy is 58.8%, and the global loss is 1.65.\n",
      "Global Round :3, the global accuracy is 59.0%, and the global loss is 1.58.\n",
      "Global Round :4, the global accuracy is 60.0%, and the global loss is 1.57.\n",
      "Global Round :5, the global accuracy is 60.2%, and the global loss is 1.56.\n",
      "Global Round :6, the global accuracy is 61.3%, and the global loss is 1.55.\n",
      "Global Round :7, the global accuracy is 62.0%, and the global loss is 1.57.\n",
      "Global Round :8, the global accuracy is 62.3%, and the global loss is 1.55.\n",
      "Global Round :9, the global accuracy is 62.3%, and the global loss is 1.52.\n",
      "Global Round :10, the global accuracy is 63.2%, and the global loss is 1.51.\n",
      "Global Round :11, the global accuracy is 64.2%, and the global loss is 1.51.\n",
      "Global Round :12, the global accuracy is 64.8%, and the global loss is 1.49.\n",
      "Global Round :13, the global accuracy is 65.0%, and the global loss is 1.5.\n",
      "Global Round :14, the global accuracy is 66.5%, and the global loss is 1.45.\n",
      "Global Round :15, the global accuracy is 66.5%, and the global loss is 1.5.\n",
      "Global Round :16, the global accuracy is 68.0%, and the global loss is 1.51.\n",
      "Global Round :17, the global accuracy is 68.0%, and the global loss is 1.48.\n",
      "Global Round :18, the global accuracy is 68.8%, and the global loss is 1.45.\n",
      "Global Round :19, the global accuracy is 67.8%, and the global loss is 1.46.\n"
     ]
    }
   ],
   "source": [
    "federal_train_avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还原模型定义\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(132, 64)\n",
    "        self.fc2 = nn.Linear(64, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缓解方案2：每轮迭代随机选取2个客户端参与训练，准确率波动比较大，准确率最高可以到70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round :0, the global accuracy is 41.2%, and the global loss is 1.82.\n",
      "Global Round :1, the global accuracy is 41.0%, and the global loss is 1.77.\n",
      "Global Round :2, the global accuracy is 38.0%, and the global loss is 1.72.\n",
      "Global Round :3, the global accuracy is 51.7%, and the global loss is 1.71.\n",
      "Global Round :4, the global accuracy is 43.5%, and the global loss is 1.7.\n",
      "Global Round :5, the global accuracy is 56.0%, and the global loss is 1.6.\n",
      "Global Round :6, the global accuracy is 47.0%, and the global loss is 1.66.\n",
      "Global Round :7, the global accuracy is 52.0%, and the global loss is 1.63.\n",
      "Global Round :8, the global accuracy is 54.8%, and the global loss is 1.57.\n",
      "Global Round :9, the global accuracy is 57.2%, and the global loss is 1.53.\n",
      "Global Round :10, the global accuracy is 58.5%, and the global loss is 1.57.\n",
      "Global Round :11, the global accuracy is 45.5%, and the global loss is 1.67.\n",
      "Global Round :12, the global accuracy is 45.0%, and the global loss is 1.65.\n",
      "Global Round :13, the global accuracy is 59.0%, and the global loss is 1.56.\n",
      "Global Round :14, the global accuracy is 61.0%, and the global loss is 1.55.\n",
      "Global Round :15, the global accuracy is 60.5%, and the global loss is 1.52.\n",
      "Global Round :16, the global accuracy is 50.7%, and the global loss is 1.61.\n",
      "Global Round :17, the global accuracy is 61.0%, and the global loss is 1.51.\n",
      "Global Round :18, the global accuracy is 60.5%, and the global loss is 1.51.\n",
      "Global Round :19, the global accuracy is 61.0%, and the global loss is 1.5.\n",
      "Global Round :20, the global accuracy is 56.0%, and the global loss is 1.54.\n",
      "Global Round :21, the global accuracy is 61.5%, and the global loss is 1.5.\n",
      "Global Round :22, the global accuracy is 63.0%, and the global loss is 1.49.\n",
      "Global Round :23, the global accuracy is 61.0%, and the global loss is 1.48.\n",
      "Global Round :24, the global accuracy is 60.0%, and the global loss is 1.49.\n",
      "Global Round :25, the global accuracy is 63.2%, and the global loss is 1.5.\n",
      "Global Round :26, the global accuracy is 57.8%, and the global loss is 1.54.\n",
      "Global Round :27, the global accuracy is 54.0%, and the global loss is 1.63.\n",
      "Global Round :28, the global accuracy is 62.0%, and the global loss is 1.48.\n",
      "Global Round :29, the global accuracy is 61.8%, and the global loss is 1.5.\n",
      "Global Round :30, the global accuracy is 60.8%, and the global loss is 1.47.\n",
      "Global Round :31, the global accuracy is 57.8%, and the global loss is 1.52.\n",
      "Global Round :32, the global accuracy is 57.8%, and the global loss is 1.46.\n",
      "Global Round :33, the global accuracy is 61.3%, and the global loss is 1.5.\n",
      "Global Round :34, the global accuracy is 61.5%, and the global loss is 1.49.\n",
      "Global Round :35, the global accuracy is 65.8%, and the global loss is 1.47.\n",
      "Global Round :36, the global accuracy is 55.2%, and the global loss is 1.55.\n",
      "Global Round :37, the global accuracy is 54.0%, and the global loss is 1.62.\n",
      "Global Round :38, the global accuracy is 63.7%, and the global loss is 1.48.\n",
      "Global Round :39, the global accuracy is 67.2%, and the global loss is 1.43.\n",
      "Global Round :40, the global accuracy is 65.0%, and the global loss is 1.42.\n",
      "Global Round :41, the global accuracy is 64.5%, and the global loss is 1.46.\n",
      "Global Round :42, the global accuracy is 62.3%, and the global loss is 1.48.\n",
      "Global Round :43, the global accuracy is 65.5%, and the global loss is 1.43.\n",
      "Global Round :44, the global accuracy is 64.8%, and the global loss is 1.44.\n",
      "Global Round :45, the global accuracy is 72.2%, and the global loss is 1.4.\n",
      "Global Round :46, the global accuracy is 71.0%, and the global loss is 1.42.\n",
      "Global Round :47, the global accuracy is 68.5%, and the global loss is 1.44.\n",
      "Global Round :48, the global accuracy is 66.2%, and the global loss is 1.43.\n",
      "Global Round :49, the global accuracy is 66.0%, and the global loss is 1.45.\n"
     ]
    }
   ],
   "source": [
    "def federal_train_avg_sample_the_client():\n",
    "  global_model = MyModel().to(device)\n",
    "  # 因为每次只有两个客户端参与，所以rounds变成了50\n",
    "  global_rounds = 50\n",
    "  local_epochs = 5\n",
    "  for round_idx in range(global_rounds):\n",
    "      local_weights = []\n",
    "      local_losses = []\n",
    "      global_acc = []\n",
    "\n",
    "      for user_index in np.random.choice(range(user_num), size=2, replace=False):\n",
    "          model_weights, loss = local_trainer(train_data_list[user_index], copy.deepcopy(global_model), round_idx, device, local_epochs, batchsize)\n",
    "          local_weights.append(copy.deepcopy(model_weights))\n",
    "          local_losses.append(loss)\n",
    "\n",
    "      global_weight = average_weights(local_weights)\n",
    "      global_model.load_state_dict(global_weight)\n",
    "      test_acc, test_loss = inference(global_model, test_loader)\n",
    "      print('Global Round :{}, the global accuracy is {:.3}%, and the global loss is {:.3}.'.format(round_idx, 100 * test_acc, test_loss))\n",
    "\n",
    "federal_train_avg_sample_the_client()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
